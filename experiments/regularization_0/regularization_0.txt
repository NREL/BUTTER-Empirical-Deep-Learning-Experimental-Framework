
mamba install python=3.9 pandas jupyter matplotlib scikit-learn statsmodels seaborn pip scipy numpy sqlalchemy psycopg2 pyarrow tensorflow-gpu=2.7 cudnn=8 simplejson --force-reinstall
squeue -o "%.10i %9P %Q %60j %10u %.2t %.10M %.6D %.4C %10R %o" | less

squeue -u ctripp -o "%.10i %9P %Q %60j %10u %.2t %.10M %.6D %.4C %10R %o"



â€‹[12:04 PM] Bendl, Kurt
source /nopt/nrel/apps/210929a/myenv.2110041605
source /nopt/nrel/apps/210929a/myenv.2110041605

https://nrel.github.io/HPC/Documentation/Systems/Vermillion/modules/



from tensorflow.python.client import device_lib

def get_available_gpus():
    local_device_protos = device_lib.list_local_devices()
    return [x.name for x in local_device_protos if x.device_type == 'GPU']

get_available_gpus()





for i in {1..50}; do sbatch -J gs12_4 -N4 -t2880 --account=dmpapps --gres=gpu:2 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp fixed_3k_1 "[[0,6,0,1,4650], [6,12,0,1,4650], [12,18,0,1,4650], [18,24,1,2,4650], [24,30,1,2,4650], [30,36,1,2,4650]]"; done


sbatch -J g_0 -N1 -t2880 --account=dmpapps --gres=gpu:2 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,0,1,4650], [6,8,1,2,4650], [8,10,1,2,4650], [10,12,1,2,4650], [12, 18, 0, 0, 0], [18, 24, 0, 0, 0], [24, 30, 0, 0, 0], [30, 36, 0, 0, 0]]"


for i in {1..10}; do sbatch -J a_0 -N1 -t2880 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,4,0,0,0], [4,8,0,0,0], [8,12,0,0,0], [12,16,0,0,0], [16,20,0,0,0], [20,24,0,0,0], [24,28,0,0,0], [28,32,0,0,0], [32,36,0,0,0]]"; done

for i in {1..100}; do sbatch -J c_0 -N1 -t2880 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,4,0,0,0], [4,8,0,0,0], [8,12,0,0,0], [12,16,0,0,0], [16,20,0,0,0], [20,24,0,0,0], [24,28,0,0,0], [28,32,0,0,0], [32,36,0,0,0]]"; done
for i in {1..100}; do sbatch -J c_1 -N2 -t2880 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,4,0,0,0], [4,8,0,0,0], [8,12,0,0,0], [12,16,0,0,0], [16,20,0,0,0], [20,24,0,0,0], [24,28,0,0,0], [28,32,0,0,0], [32,36,0,0,0]]"; done
for i in {1..40}; do sbatch -J c_2 -N4 -t2880 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,4,0,0,0], [4,8,0,0,0], [8,12,0,0,0], [12,16,0,0,0], [16,20,0,0,0], [20,24,0,0,0], [24,28,0,0,0], [28,32,0,0,0], [32,36,0,0,0]]"; done
for i in {1..40}; do sbatch -J c_3 -N8 -t2880 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,4,0,0,0], [4,8,0,0,0], [8,12,0,0,0], [12,16,0,0,0], [16,20,0,0,0], [20,24,0,0,0], [24,28,0,0,0], [28,32,0,0,0], [32,36,0,0,0]]"; done
for i in {1..40}; do sbatch -J c_4 -N16 -t2880 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,4,0,0,0], [4,8,0,0,0], [8,12,0,0,0], [12,16,0,0,0], [16,20,0,0,0], [20,24,0,0,0], [24,28,0,0,0], [28,32,0,0,0], [32,36,0,0,0]]"; done
for i in {1..40}; do sbatch -J c_5 -N32 -t2880 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,4,0,0,0], [4,8,0,0,0], [8,12,0,0,0], [12,16,0,0,0], [16,20,0,0,0], [20,24,0,0,0], [24,28,0,0,0], [28,32,0,0,0], [32,36,0,0,0]]"; done
for i in {1..40}; do sbatch -J c_6 -N64 -t2880 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,4,0,0,0], [4,8,0,0,0], [8,12,0,0,0], [12,16,0,0,0], [16,20,0,0,0], [20,24,0,0,0], [24,28,0,0,0], [28,32,0,0,0], [32,36,0,0,0]]"; done
for i in {1..40}; do sbatch -J c_8 -N128 -t2880 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,4,0,0,0], [4,8,0,0,0], [8,12,0,0,0], [12,16,0,0,0], [16,20,0,0,0], [20,24,0,0,0], [24,28,0,0,0], [28,32,0,0,0], [32,36,0,0,0]]"; done

for i in {1..50}; do sbatch -J g_0 -N1 -t2880 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,0,1,4650], [6,8,1,2,4650], [8,10,1,2,4650], [10,12,1,2,4650], [12,18,0,0,0], [18,24,0,0,0], [24,30,0,0,0], [30,36,0,0,0]]"; done
for i in {1..50}; do sbatch -J g_1 -N2 -t2880 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,0,1,4650], [6,8,1,2,4650], [8,10,1,2,4650], [10,12,1,2,4650], [12,18,0,0,0], [18,24,0,0,0], [24,30,0,0,0], [30,36,0,0,0]]"; done
for i in {1..50}; do sbatch -J g_2 -N4 -t2880 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,0,1,4650], [6,8,1,2,4650], [8,10,1,2,4650], [10,12,1,2,4650], [12,18,0,0,0], [18,24,0,0,0], [24,30,0,0,0], [30,36,0,0,0]]"; done



for i in {1..50}; do sbatch -J g_0 -N1 -t2880 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,0,1,4650], [6,8,1,2,4650], [8,10,1,2,4650], [10,12,1,2,4650], [12,18,0,0,0], [18,24,0,0,0], [24,30,0,0,0], [30,36,0,0,0]]"; done
for i in {1..50}; do sbatch -J g_1 -N2 -t2880 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,0,1,4650], [6,8,1,2,4650], [8,10,1,2,4650], [10,12,1,2,4650], [12,18,0,0,0], [18,24,0,0,0], [24,30,0,0,0], [30,36,0,0,0]]"; done
for i in {1..50}; do sbatch -J g_2 -N4 -t2880 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,0,1,4650], [6,8,1,2,4650], [8,10,1,2,4650], [10,12,1,2,4650], [12,18,0,0,0], [18,24,0,0,0], [24,30,0,0,0], [30,36,0,0,0]]"; done


for i in {1..50}; do sbatch -J g_0 -N1 -t2880 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,1,4650], [3,6,0,1,4650], [6,9,0,1,4650], [9,12,1,2,4650], [12,15,1,2,4650], [15,18,1,2,4650], [12, 15, 0, 0, 0], [15, 18, 0, 0, 0], [18, 21, 0, 0, 0], [21, 24, 0, 0, 0], [24, 27, 0, 0, 0], [27, 30, 0, 0, 0], [30, 33, 0, 0, 0], [33, 36, 0, 0, 0]]"; done
for i in {1..50}; do sbatch -J g_1 -N2 -t2880 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,1,4650], [3,6,0,1,4650], [6,9,0,1,4650], [9,12,1,2,4650], [12,15,1,2,4650], [15,18,1,2,4650], [12, 15, 0, 0, 0], [15, 18, 0, 0, 0], [18, 21, 0, 0, 0], [21, 24, 0, 0, 0], [24, 27, 0, 0, 0], [27, 30, 0, 0, 0], [30, 33, 0, 0, 0], [33, 36, 0, 0, 0]]"; done
for i in {1..50}; do sbatch -J g_3 -N3 -t2880 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,1,4650], [3,6,0,1,4650], [6,9,0,1,4650], [9,12,1,2,4650], [12,15,1,2,4650], [15,18,1,2,4650], [12, 15, 0, 0, 0], [15, 18, 0, 0, 0], [18, 21, 0, 0, 0], [21, 24, 0, 0, 0], [24, 27, 0, 0, 0], [27, 30, 0, 0, 0], [30, 33, 0, 0, 0], [33, 36, 0, 0, 0]]"; done
for i in {1..50}; do sbatch -J g_4 -N4 -t2880 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,1,4650], [3,6,0,1,4650], [6,9,0,1,4650], [9,12,1,2,4650], [12,15,1,2,4650], [15,18,1,2,4650], [12, 15, 0, 0, 0], [15, 18, 0, 0, 0], [18, 21, 0, 0, 0], [21, 24, 0, 0, 0], [24, 27, 0, 0, 0], [27, 30, 0, 0, 0], [30, 33, 0, 0, 0], [33, 36, 0, 0, 0]]"; done

for i in {1..128}; do sbatch -J c_1 -t2880 -N1 -c30 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..128}; do sbatch -J c_2 -t2880 -N2 -c30 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..64}; do sbatch -J c_4 -t2880 -N4 -c30 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..64}; do sbatch -J c_8 -t2880 -N8 -c30 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..64}; do sbatch -J c_16 -t2880 -N16 -c30 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..64}; do sbatch -J c_32 -t2880 -N32 -c30 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..64}; do sbatch -J c_64 -t2880 -N64 -c30 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done


for i in {1..32}; do sbatch -J c_256 -t2880 -N256 -c30 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..32}; do sbatch -J c_128 -t2880 -N128 -c30 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..64}; do sbatch -J c_512 -t2880 -N512 -c30 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..64}; do sbatch -J c_384 -t2880 -N384 -c30 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done

for i in {1..32}; do sbatch -J lg_1 -N1 -t14400 --gres=gpu:2 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,1,2,4650], [6,8,1,2,4650], [8,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..32}; do sbatch -J lg_2 -N2 -t14400 --gres=gpu:2 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,1,2,4650], [6,8,1,2,4650], [8,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..32}; do sbatch -J lg_4 -N4 -t14400 --gres=gpu:2 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,1,2,4650], [6,8,1,2,4650], [8,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done

for i in {1..32}; do sbatch -J lgx_1 -N1 -t14400 --gres=gpu:2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,1,4650], [3,6,0,1,4650], [6,9,0,1,4650], [9,12,1,2,4650], [12,15,1,2,4650], [15,18,1,2,4650], [12, 15, 0, 0, 0], [15, 18, 0, 0, 0], [18, 21, 0, 0, 0], [21, 24, 0, 0, 0], [24, 27, 0, 0, 0], [27, 30, 0, 0, 0], [30, 33, 0, 0, 0], [33, 36, 0, 0, 0]]"; done
for i in {1..32}; do sbatch -J lgx_2 -N2 -t14400 --gres=gpu:2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,1,4650], [3,6,0,1,4650], [6,9,0,1,4650], [9,12,1,2,4650], [12,15,1,2,4650], [15,18,1,2,4650], [12, 15, 0, 0, 0], [15, 18, 0, 0, 0], [18, 21, 0, 0, 0], [21, 24, 0, 0, 0], [24, 27, 0, 0, 0], [27, 30, 0, 0, 0], [30, 33, 0, 0, 0], [33, 36, 0, 0, 0]]"; done
for i in {1..32}; do sbatch -J lgx_4 -N4 -t14400 --gres=gpu:2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,1,4650], [3,6,0,1,4650], [6,9,0,1,4650], [9,12,1,2,4650], [12,15,1,2,4650], [15,18,1,2,4650], [12, 15, 0, 0, 0], [15, 18, 0, 0, 0], [18, 21, 0, 0, 0], [21, 24, 0, 0, 0], [24, 27, 0, 0, 0], [27, 30, 0, 0, 0], [30, 33, 0, 0, 0], [33, 36, 0, 0, 0]]"; done


for i in {1..32}; do sbatch -J lc_1 -t14400 -N1 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J lc_4 -t14400 -N4 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J lc_8 -t14400 -N8 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J lc_16 -t14400 -N16 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J lc_32 -t14400 -N32 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J lc_64 -t14400 -N64 --account=dmpscale slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done


for i in {1..32}; do sbatch -J lcx_1 -t14400 -N1 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..32}; do sbatch -J lcx_2 -t14400 -N2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J lcx_4 -t14400 -N4 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J lcx_8 -t14400 -N8 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J lcx_16 -t14400 -N16 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J lcx_32 -t14400 -N32 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done


for i in {1..32}; do sbatch -J cx_1 -t2880 -N1 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..32}; do sbatch -J cx_2 -t2880 -N2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J cx_4 -t2880 -N4 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J cx_8 -t2880 -N8 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J cx_16 -t2880 -N16 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J cx_32 -t2880 -N32 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J cx_64 -t2880 -N64 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J cx_128 -t2880 -N128 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..16}; do sbatch -J cx_128 -t2880 -N256 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done




for i in {1..2}; do sbatch -J cxs_1 -t1440 -N1 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..2}; do sbatch -J cxs_2 -t1440 -N4 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..2}; do sbatch -J cxs_4 -t1440 -N4 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..2}; do sbatch -J cxs_8 -t1440 -N8 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..2}; do sbatch -J cxs_16 -t1440 -N16 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done

for i in {1..2}; do sbatch -J lgs_1 -N1 -t1440 --gres=gpu:2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,1,2,4650], [6,8,1,2,4650], [8,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..2}; do sbatch -J lgs_2 -N2 -t1440 --gres=gpu:2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,1,2,4650], [6,8,1,2,4650], [8,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..2}; do sbatch -J lgs_4 -N4 -t1440 --gres=gpu:2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,1,2,4650], [6,8,1,2,4650], [8,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done

for i in {1..2}; do sbatch -J lgx_1 -N1 -t4000 --gres=gpu:2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,1,2,4650], [6,8,1,2,4650], [8,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..2}; do sbatch -J lgx_2 -N2 -t4000 --gres=gpu:2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,1,2,4650], [6,8,1,2,4650], [8,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..2}; do sbatch -J lgx_4 -N4 -t4000 --gres=gpu:2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,1,4650], [2,4,0,1,4650], [4,6,1,2,4650], [6,8,1,2,4650], [8,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done


for i in {1..2}; do sbatch -J sg_1 -N1 -t1440 --gres=gpu:2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,2,0,1,4650], [2,4,0,1,4650], [4,6,1,2,4650], [6,8,1,2,4650], [8,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0], [36,39,0,0,0]]"; done


for i in {1..2}; do sbatch -J cxx_1 -t4000 -N1 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..2}; do sbatch -J cxx_2 -t4000 -N2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done
for i in {1..2}; do sbatch -J cxx_4 -t4000 -N4 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0], [30,33,0,0,0], [33,36,0,0,0]]"; done






sbatch -J v_28 -t1440 -n1 -c28 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,0,0]]"

for i in {1..400}; do sbatch -J v_0 -t1440 -n1 -c4 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,4,0,0,0]]"; done

for i in {1..400}; do sbatch -J v_0 -t1440 -n1 -c2 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,2,0,0,0]]"; done

for i in {1..400}; do sbatch -J vg_0 -t1440 -n1 -c8 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,1,0,1,4650], [1,2,0,1,4650], [2,3,0,1,4650], [3,4,0,1,4650], [4,5,0,1,4650], [5,6,0,1,4650], [5,6,0,1,4650], [5,6,0,1,4650]]"; done

sbatch --partition=gpu -J vg_0 -t1440 -n1 -c29 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,1,0,1,4445], [1,2,0,1,4445], [2,3,0,1,4445], [3,4,0,1,4445], [4,5,0,1,4445], [5,6,0,1,4445], [5,6,0,1,4445], [5,6,0,1,4445], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,29,0,0,0]]"
sbatch --partition=gpu -J vg_0 -t1440 -n1 -c29 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0,1,0,1,4445], [1,2,0,1,4445], [2,3,0,1,4445], [3,4,0,1,4445], [4,5,0,1,4445], [5,7,0,0,0], [7,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,29,0,0,0]]"

for i in {1..50}; do sbatch --partition=gpu -J g_1 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0], [0,2,0,1,6000], [2,4,0,1,6000], [4,6,0,1,6000], [6,8,0,1,6000], [8,10,0,1,6000], [10,12,0,0,0], [12,14,0,0,0], [14,16,0,0,0], [16,18,0,0,0], [18,20,0,0,0], [20,22,0,0,0], [22,24,0,0,0], [24,26,0,0,0], [26,28,0,0,0], [28,30,0,0,0]]"; done
for i in {1..200}; do sbatch -J c_1 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[[0], [0,2,0,0,0], [2,4,0,0,0], [4,6,0,0,0], [6,8,0,0,0], [8,10,0,0,0], [10,12,0,0,0], [12,14,0,0,0], [14,16,0,0,0], [16,18,0,0,0], [18,20,0,0,0], [20,22,0,0,0], [22,24,0,0,0], [24,26,0,0,0], [26,28,0,0,0], [28,30,0,0,0]]"; done

for i in {1..200}; do sbatch -J c_1 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,10,0,0,0], [10,20,0,0,0], [20,30,0,0,0]]"; done

for i in {1..200}; do sbatch -J c_1 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,30,0,0,0]]"; done

for i in {1..100}; do sbatch -J c_2 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,3,0,0,0], [3,6,0,0,0], [6,9,0,0,0], [9,12,0,0,0], [12,15,0,0,0], [15,18,0,0,0], [18,21,0,0,0], [21,24,0,0,0], [24,27,0,0,0], [27,30,0,0,0]]"; done


for i in {1..200}; do sbatch -J c_3 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0, 2, 0, 0, 0], [2, 4, 0, 0, 0], [4, 6, 0, 0, 0], [6, 8, 0, 0, 0], [8, 10, 0, 0, 0], [10, 12, 0, 0, 0], [12, 14, 0, 0, 0], [14, 16, 0, 0, 0], [16, 18, 0, 0, 0], [18, 20, 0, 0, 0], [20, 22, 0, 0, 0], [22, 24, 0, 0, 0], [24, 26, 0, 0, 0], [26, 28, 0, 0, 0], [28, 30, 0, 0, 0], [30, 32, 0, 0, 0], [32, 34, 0, 0, 0], [34, 36, 0, 0, 0]]"; done




for i in {1..200}; do sbatch --partition=gpu -J g_1 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,1,0,1,10000], [1,2,0,1,10000], [2,3,0,1,10000], [3,30,0,0,0]]"; done
sbatch --partition=gpu -J g_1 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,1,0,1,9000], [1,2,0,1,9000], [2,3,0,1,9000], [3,30,0,0,0]]"


for i in {1..100}; do sbatch -J c_4 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [[0, 4, 0, 0, 0], [4, 8, 0, 0, 0], [8, 12, 0, 0, 0], [12, 16, 0, 0, 0], [16, 20, 0, 0, 0], [20, 24, 0, 0, 0], [24, 28, 0, 0, 0], [28, 32, 0, 0, 0]]]"; done
for i in {1..100}; do sbatch -J c_5 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0, 6, 0, 0, 0], [6, 12, 0, 0, 0], [12, 18, 0, 0, 0], [18, 24, 0, 0, 0], [24, 30, 0, 0, 0], [30, 36, 0, 0, 0], [36, 42, 0, 0, 0]]"; done

for i in {1..1}; do sbatch -J c_5 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0, 8, 0, 0, 0], [6, 12, 0, 0, 0], [12, 18, 0, 0, 0], [18, 24, 0, 0, 0], [24, 30, 0, 0, 0], [30, 36, 0, 0, 0], [36, 42, 0, 0, 0]]"; done
for i in {1..100}; do sbatch -J c_6 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0, 8, 0, 0, 0], [8, 16, 0, 0, 0], [16, 24, 0, 0, 0], [24, 32, 0, 0, 0], [32, 40, 0, 0, 0], [40, 48, 0, 0, 0], [48, 56, 0, 0, 0], [56, 64, 0, 0, 0], [64, 72, 0, 0, 0], [72, 80, 0, 0, 0]]"; done

for i in {1..50}; do sbatch --partition=gpu -J g_2 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,2,0,1,8000], [2,4,0,1,8000], [4,6,0,1,8000], [6,8,0,1,8000], [6, 12, 0, 0, 0], [12, 18, 0, 0, 0], [18, 24, 0, 0, 0], [24, 30, 0, 0, 0]]"; done
for i in {1..50}; do sbatch --partition=gpu -J g_3 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,4,0,1,8000], [4,8,0,1,8000], [8,16,0,1,8000], [16,20,0,1,8000], [0, 8, 0, 0, 0], [8, 16, 0, 0, 0], [16, 24, 0, 0, 0], [24, 32, 0, 0, 0]]"; done

for i in {1..100}; do sbatch --partition=gpu -J g_4 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0,2,0,1,5000], [2,4,0,1,5000], [4,6,0,1,5000], [6,8,0,1,5000], [8,10,0,1,5000], [10, 16, 0, 0, 0], [16, 24, 0, 0, 0], [24, 30, 0, 0, 0]]"; done
for i in {1..400}; do sbatch -J c_4 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 "[0, [0, 6, 0, 0, 0], [6, 12, 0, 0, 0], [12, 18, 0, 0, 0], [18, 24, 0, 0, 0], [24, 30, 0, 0, 0]]"; done




sbatch --partition=gpu -J g_4 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1
for i in {1..50}; do sbatch --partition=gpu -J g_4 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done

for i in {1..50}; do sbatch --partition=gpu -J g_4 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done

for i in {1..200}; do sbatch -J c_5 -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 ; done




sbatch -J d_1 -t30 -N1 --account=dmpapps --partition=debug slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1
sbatch -J d_1 -t30 -N1 --account=dmpapps --partition=debug --gres=gpu:2 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1


squeue -u ctripp -A dmpapps -h | grep -P ".*\s+c_\d\s+.*\(.*\)" | awk '{print $1}' | xargs scancel

squeue -u ctripp -A dmpapps -h | grep -P "\s+g_\d+\s+.*" | awk '{print $1}' | xargs scancel
squeue -u ctripp -A dmpapps -h | grep -P "\s+gs12_1\s+.*" | awk '{print $1}' | xargs scontrol release

squeue -u ctripp -h | grep -P ".*\s+flx_\s+.*\(.*\)" | awk '{print $1}' | xargs scancel




sbatch -J ct_1 -q regular --time=2-00:00:00 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --account=m3772 --constraint=haswell slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1

sbatch -J ct_1 -q regular --time=2-00:00:00 --nodes=1 --ntasks-per-node=1 --cpus-per-task=32 --account=m3772 --constraint=haswell slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1

sbatch -J pt_1 -q regular --time=6:00:00 --nodes=1 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -q regular -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1
sbatch -J pt_128_1 -q regular --time=6:00:00 --nodes=128 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -q regular -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1

sbatch -J pt_1 -q interactive --time=2:00:00 --nodes=1 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -q regular -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1


for i in {1..100}; do sbatch -J gpu_1 -N1 -t2880 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..64}; do sbatch -J cpu_2 -N2 -t2880 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done

for i in {1..40}; do sbatch -J lgx_1 -N1 -t4000 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..20}; do sbatch -J lgx_2 -N2 -t4000 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..20}; do sbatch -J lgx_3 -N3 -t4000 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..20}; do sbatch -J lgx_4 -N4 -t4000 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..20}; do sbatch -J lgx_5 -N5 -t4000 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..20}; do sbatch -J lgx_6 -N6 -t4000 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..32}; do sbatch -J lgx_8 -N8 -t4000 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done


for i in {1..16}; do sbatch -J lx_128 -N128 -t4000 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done

for i in {1..16}; do sbatch -J x_512 -N512 -t2880 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..16}; do sbatch -J x_256 -N256 -t2880 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..16}; do sbatch -J x_900 -N900 -t2880 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done

for i in {1..16}; do sbatch -J xs_512 -N512 -t1440 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..16}; do sbatch -J xs_256 -N256 -t1440 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..16}; do sbatch -J xs_900 -N900 -t1440 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done


for i in {0..7}; do for j in {0..$i}; do echo $((j)); done; done;

{
max_power=3;
min_power=0;
jobs_at_max=16;
num_jobs=jobs_at_max;
for ((power=$max_power; power>=$min_power; power--)); do
  num_nodes=$((2**power));
  for ((i=0; i<$num_jobs;i++)); do
    t=$((2 + (i % 5)))
    sbatch -J pt_$((num_nodes))_$((t)) -q regular --time=6:00:00 --nodes=$((num_nodes)) --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1;
  done;
  num_jobs=$((num_jobs * 2));
done;
}

{
max_power=2;
min_power=0;
jobs_per_power=128;
for ((power=$max_power; power>=$min_power; power--)); do
  num_nodes=$((2**power));
  for ((i=0; i<$jobs_per_power;i++)); do
    t=$((2 + (i % 5)))
    sbatch -J pt_$((num_nodes))_$((t)) -q regular --time=$((t)):00:00 --nodes=$((num_nodes)) --ntasks-per-node=1 --account=m3772_g --constraint=gpu --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1;
  done;
done;
}

{
max_power=2;
min_power=0;
jobs_at_max=8;
num_jobs=jobs_at_max;
for ((power=$max_power; power>=$min_power; power--)); do
  num_nodes=$((2**power));
  for ((i=0; i<$num_jobs;i++)); do
    sbatch -J pt_d_$((num_nodes)) -q debug  --time=2:00:00 --nodes=$((num_nodes)) --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1;
  done;
  num_jobs=$((num_jobs * 2));
done;
}



for i in {0..128}; do sbatch -J pt_128 -q regular --time=6:00:00 --nodes=128 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pt_128 -q regular --time=6:00:00 --nodes=128 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pt_96 -q regular --time=6:00:00 --nodes=96 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pt_72 -q regular --time=6:00:00 --nodes=72 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..64}; do sbatch -J pt_64 -q regular --time=6:00:00 --nodes=64 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..64}; do sbatch -J pt_32 -q regular --time=6:00:00 --nodes=32 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..64}; do sbatch -J pt_16 -q regular --time=6:00:00 --nodes=16 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..64}; do sbatch -J pt_8 -q regular --time=6:00:00 --nodes=8 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..128}; do sbatch -J pt_4 -q regular --time=6:00:00 --nodes=4 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..128}; do sbatch -J pt_2 -q regular --time=6:00:00 --nodes=2 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..128}; do sbatch -J pt_1 -q regular --time=6:00:00 --nodes=1 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;


for i in {0..8}; do sbatch -J pt_128 -q regular_ss10 --time=12:00:00 --nodes=128 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..8}; do sbatch -J pt_96 -q regular_ss10 --time=12:00:00 --nodes=96 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..8}; do sbatch -J pt_72 -q regular_ss10 --time=12:00:00 --nodes=72 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..8}; do sbatch -J pt_64 -q regular_ss10 --time=12:00:00 --nodes=64 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..8}; do sbatch -J pt_32 -q regular_ss10 --time=12:00:00 --nodes=32 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..8}; do sbatch -J pt_16 -q regular_ss10 --time=12:00:00 --nodes=16 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..8}; do sbatch -J pt_8 -q regular_ss10 --time=12:00:00 --nodes=8 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..8}; do sbatch -J pt_4 -q regular_ss10 --time=12:00:00 --nodes=4 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..8}; do sbatch -J pt_2 -q regular_ss10 --time=12:00:00 --nodes=2 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..8}; do sbatch -J pt_1 -q regular_ss10 --time=12:00:00 --nodes=1 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;


for i in {0..32}; do sbatch -J pt_128 -q regular --time=12:00:00 --nodes=128 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pt_96 -q regular --time=12:00:00 --nodes=96 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pt_72 -q regular --time=12:00:00 --nodes=72 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pt_64 -q regular --time=12:00:00 --nodes=64 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pt_32 -q regular --time=12:00:00 --nodes=32 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pt_16 -q regular --time=12:00:00 --nodes=16 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pt_8 -q regular --time=12:00:00 --nodes=8 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pt_4 -q regular --time=12:00:00 --nodes=4 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pt_2 -q regular --time=12:00:00 --nodes=2 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pt_1 -q regular --time=12:00:00 --nodes=1 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;

srun  -J pt_1 -q interactive --time=2:00:00 --nodes=1 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 --pty bash -i 


for i in {0..128}; do sbatch -J d_4 -q debug --time=2:00:00 --nodes=4 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
sbatch -J j_1 -q jupyter --time=6:00:00 --nodes=4 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1

for i in {0..32}; do sbatch -J pts_128 -q regular_ss11 --time=12:00:00 --nodes=128 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pts_96 -q regular_ss11 --time=12:00:00 --nodes=96 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pts_72 -q regular_ss11 --time=12:00:00 --nodes=72 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..64}; do sbatch -J pts_64 -q regular_ss11 --time=12:00:00 --nodes=64 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..64}; do sbatch -J pts_32 -q regular_ss11 --time=12:00:00 --nodes=32 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..64}; do sbatch -J pts_16 -q regular_ss11 --time=12:00:00 --nodes=16 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pts_8 -q regular_ss11 --time=12:00:00 --nodes=8 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pts_4 -q regular_ss11 --time=12:00:00 --nodes=4 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pts_2 -q regular_ss11 --time=12:00:00 --nodes=2 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pts_1 -q regular_ss11 --time=12:00:00 --nodes=1 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;



for i in {0..16}; do sbatch -J pts5_8 -q regular_ss11 --time=5:00:00 --nodes=8 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pts5_4 -q regular_ss11 --time=5:00:00 --nodes=4 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pts5_2 -q regular_ss11 --time=5:00:00 --nodes=2 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pts5_1 -q regular_ss11 --time=5:00:00 --nodes=1 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;


for i in {0..16}; do sbatch -J pc_128 -q regular --time=6:00:00 --nodes=128 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_96 -q regular --time=6:00:00 --nodes=96 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pc_72 -q regular --time=6:00:00 --nodes=72 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..32}; do sbatch -J pc_64 -q regular --time=6:00:00 --nodes=64 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_32 -q regular --time=6:00:00 --nodes=32 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_16 -q regular --time=6:00:00 --nodes=16 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_8 -q regular --time=6:00:00 --nodes=8 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_4 -q regular --time=6:00:00 --nodes=4 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_2 -q regular --time=6:00:00 --nodes=2 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_1 -q regular --time=6:00:00 --nodes=1 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;


for i in {0..16}; do sbatch -J pc_12_128 -q regular --time=12:00:00 --nodes=128 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_12_96 -q regular --time=12:00:00 --nodes=96 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_12_72 -q regular --time=12:00:00 --nodes=72 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_12_64 -q regular --time=12:00:00 --nodes=64 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_12_32 -q regular --time=12:00:00 --nodes=32 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_12_16 -q regular --time=12:00:00 --nodes=16 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_12_8 -q regular --time=12:00:00 --nodes=8 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_12_4 -q regular --time=12:00:00 --nodes=4 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_12_2 -q regular --time=12:00:00 --nodes=2 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
for i in {0..16}; do sbatch -J pc_12_1 -q regular --time=12:00:00 --nodes=1 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 256  slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;



sbatch -J pc_1 -q regular --time=6:00:00 --nodes=1 --ntasks-per-node=1 --account=m3772 --constraint=cpu -c 128 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1





sbatch -J ck_0 -q regular --time=48:00:00 --nodes=1 --ntasks-per-node=1 --account=m3772 --constraint=knl --cpus-per-task=272 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1

for i in {0..128}; do sbatch -J flx_1 -N 1 --account=m3772_g -c 128 -n 1 --gpus-per-task=4 flex_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;


sbatch -J pt_d -q debug  --time=2:00:00 --nodes=1 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1
for i in {0..10}; do sbatch -J pt_d -q debug  --time=2:00:00 --nodes=4 --ntasks-per-node=1 --account=m3772_g --constraint=gpu -c 128 --gpus-per-task=4 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done;
salloc --nodes 4 --qos interactive --time 4:00:00 --constraint gpu --gpus-per-task 4 -c 128 --ntasks-per-node 1 --account=m3772_g srun slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1

sbatch -J ct_r_h_1 -q regular --time=48:00:00 --nodes=1 --ntasks-per-node=1 --account=m3772 --constraint=haswell --cpus-per-task=32 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1


sbatch -J ct_d -q debug --time=30:00 --nodes=1 --ntasks-per-node=1 --account=m3772 --constraint=haswell --cpus-per-task=32 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1
sbatch -J t_knl_l_4 -q low --time=48:00:00 --nodes=4 --ntasks-per-node=1 --account=m3772 --constraint=knl --cpus-per-task=272 slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1


sbatch -J ckf_1 -q flex --time-min=4:00:00 --time=48:00:00 --comment=100:00:00 -N 1 -A m3772 -C knl --mail-user=charles.tripp@nrel.gov flex_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1

squeue -u ctripp -h | grep -P "\sgpu\s.*\sPD\s" | awk '{print $1}' | xargs scontrol hold

squeue -u ctripp -h | grep -P "gpu" | awk '{print $1}' | xargs scontrol hold
squeue -u ctripp -h | grep -P "gpu" | awk '{print $1}' | xargs scontrol release
squeue -u ctripp -h | grep -P ".*\s+\([^)]+\)" | awk '{print $1}' | xargs scontrol hold
squeue -u ctripp -h | grep -P ".*\s+\([^)]+\)" | awk '{print $1}' | xargs scontrol release
squeue -u ctripp -h | awk '{print $1}' | xargs scontrol hold


squeue -u ctripp -h | grep -P "\spc_128\s.*\s\([^)]+\)" | awk '{print $1}' | xargs scontrol hold

squeue -u ctripp -h | grep -P ".*\s+\(JobHeldUser\)" | awk '{print $1}' | xargs scontrol release
squeue -u ctripp -h | grep -P ".*\s+R\s+" | awk '{print $1}' | xargs scancel

squeue -u ctripp -h | grep -P ".*\s+gc_g_1\s+" | awk '{print $1}' | xargs scancel


squeue -u ctripp -h | grep -P ".*\s+gc_.+\s+" | awk '{print $1}' | xargs scancel



for i in {1..128}; do sbatch --partition=gpu -J gpu -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 ; done
for i in {1..256}; do sbatch -J large -t1440 -n1 -c60 --partition=lg --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 ; done
for i in {1..1024}; do sbatch -J small -t1440 -n1 -c30 --account=dmpapps slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1 ; done




for i in {1..40}; do sbatch -J gx_1 -N1 -t2880 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..40}; do sbatch -J gx_2 -N2 -t2880 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..40}; do sbatch -J gx_3 -N3 -t2880 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..40}; do sbatch -J gx_4 -N4 -t2880 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..40}; do sbatch -J gx_8 -N8 -t2880 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..10}; do sbatch -J gx_10 -N10 -t2880 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done


for i in {1..512}; do sbatch -J gc_g_1 -N1 -t2880 --gres=gpu:2 --account=gcomp --qos=standby ./custom_gpu_run_script.sh 1 2 0 0,1 python -u -m dmp.jobqueue_interface.worker_manager python -u -m dmp.jobqueue_interface.worker 0 1 0 2 0 2 13312 dmp 2 0 0,1 ; done

for i in {1..128}; do sbatch -J gc_gs_1 -N1 -t1440 --gres=gpu:2 --account=gcomp --qos=standby ./custom_gpu_run_script.sh 1 2 0 0,1 python -u -m dmp.jobqueue_interface.worker_manager python -u -m dmp.jobqueue_interface.worker 0 1 0 2 0 2 13312 dmp 2 0 0,1 ; done
for i in {1..32}; do sbatch -J gc_gl_1 -N1 -t14250 --gres=gpu:2 --account=gcomp --qos=standby ./custom_gpu_run_script.sh 1 2 0 0,1 python -u -m dmp.jobqueue_interface.worker_manager python -u -m dmp.jobqueue_interface.worker 0 1 0 2 0 2 13312 dmp 2 0 0,1 ; done


0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35


for i in {1..1024}; do sbatch -J gc_c_1 -N1 -t2880 --account=gcomp --qos=standby  ./custom_cpu_run_script.sh 2 36 0,1 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35 python -u -m dmp.jobqueue_interface.worker_manager python -u -m dmp.jobqueue_interface.worker 0 2 0 36 0 0 0 dmp 3 0,1 0,1 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35 ; done

for i in {1..1024}; do sbatch -J gc_c_1 -N1 -t2880 --account=gcomp --qos=standby  ./custom_cpu_run_script.sh 2 36 0,1 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35 python -u -m dmp.jobqueue_interface.worker_manager python -u -m dmp.jobqueue_interface.worker 0 2 0 36 0 0 0 dmp 3 0,1 0,1 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35 ; done

for i in {1..1024}; do sbatch -J gc_cs_1 -N1 -t1440 --account=gcomp --qos=standby  ./custom_cpu_run_script.sh 2 36 0,1 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35 python -u -m dmp.jobqueue_interface.worker_manager python -u -m dmp.jobqueue_interface.worker 0 2 0 36 0 0 0 dmp 3 0,1 0,1 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35 ; done


for i in {1..128}; do sbatch -J gc_cxl_1 -N1 -t14250 --account=gcomp --qos=standby ./custom_cpu_run_script.sh 2 36 0,1 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35 python -u -m dmp.jobqueue_interface.worker_manager python -u -m dmp.jobqueue_interface.worker 0 2 0 36 0 0 0 dmp 3 0,1 0,1 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35 ; done

for i in {1..128}; do sbatch -J gc_cxl_2 -N1 -t14250 --account=gcomp --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done

for i in {1..40}; do sbatch -J gx_10 -N10 -t2880 --gres=gpu:2 --account=dmpscale --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done

for i in {1..8}; do sbatch -J lgx_1 -N1 -t14400 --gres=gpu:2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..8}; do sbatch -J lgx_2 -N2 -t14400 --gres=gpu:2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..8}; do sbatch -J lgx_4 -N4 -t14400 --gres=gpu:2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
for i in {1..8}; do sbatch -J lgx_8 -N8 -t14400 --gres=gpu:2 --account=dmpapps --qos=standby slurm_job_runner.sh python -m dmp.jq.jq_node_manager dmp 1; done
