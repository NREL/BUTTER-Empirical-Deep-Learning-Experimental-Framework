{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy\n",
    "import pandas\n",
    "import io\n",
    "import uuid\n",
    "import psycopg.sql\n",
    "import pyarrow\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.parquet \n",
    "import jobqueue\n",
    "from jobqueue.connection_manager import ConnectionManager\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from typing import Callable, List\n",
    "\n",
    "from psycopg import sql\n",
    "\n",
    "import dmp.keras_interface.model_serialization as model_serialization\n",
    "from dmp.task.experiment.training_experiment.training_epoch import TrainingEpoch\n",
    "from dmp.postgres_interface.element.column import Column\n",
    "from dmp.postgres_interface.element.table import Table\n",
    "from dmp.postgres_interface.element.column_group import ColumnGroup\n",
    "\n",
    "from dmp.util.butter_e_export import *\n",
    "\n",
    "pd.options.display.max_seq_items = None\n",
    "credentials = jobqueue.load_credentials(\"dmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run vars {'_name': 'run', '_columns': (), '_index': None}\n",
      "('experiment_id', 'run_timestamp', 'run_id', 'job_id', 'seed', 'slurm_job_id', 'task_version', 'num_nodes', 'num_cpus', 'num_gpus', 'gpu_memory', 'host_name', 'batch', 'run_data', 'run_history', 'run_extended_history', 'queue', 'status', 'priority', 'start_time', 'update_time', 'worker', 'error_count', 'error', 'parent', 'command')\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n"
     ]
    }
   ],
   "source": [
    "from psycopg import ClientCursor\n",
    "\n",
    "\n",
    "print(f\"run vars {vars(run)}\")\n",
    "\n",
    "columns = (\n",
    "    run\n",
    "    + ColumnGroup(*[c for c in job_status.columns if c.name != \"id\"])\n",
    "    + job_data.command\n",
    ")\n",
    "print(columns.names)\n",
    "\n",
    "\n",
    "def passthrough(row, index, value, column, data):\n",
    "    data[column.name] = value\n",
    "\n",
    "\n",
    "column_converters: List[Callable] = [passthrough for _ in columns]\n",
    "\n",
    "\n",
    "def flatten_json(json_obj, destination=None, parent_key=\"\", separator=\"_\"):\n",
    "    if isinstance(destination, dict):\n",
    "        flattened = destination\n",
    "    else:\n",
    "        flattened = {}\n",
    "\n",
    "    for key, value in json_obj.items():\n",
    "        new_key = f\"{parent_key}{separator}{key}\" if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            flattened.update(flatten_json(value, new_key, separator=separator))\n",
    "        else:\n",
    "            flattened[new_key] = value\n",
    "    return flattened\n",
    "\n",
    "\n",
    "column_converters[\n",
    "    columns.get_index_of(job_data.command)\n",
    "] = lambda row, index, value, column, data: flatten_json(value, destination=data)\n",
    "column_converters[\n",
    "    columns.get_index_of(run.run_data)\n",
    "] = lambda row, index, value, column, data: flatten_json(value, destination=data)\n",
    "\n",
    "\n",
    "def parquet_to_dataframe(row, index, value, column, data):\n",
    "    with io.BytesIO(value) as buffer:\n",
    "        data[column.name] = (\n",
    "            pyarrow.parquet.read_table(pyarrow.PythonFile(buffer, mode=\"r\"))\n",
    "            .to_pandas()\n",
    "            .sort_values(by=\"epoch\")\n",
    "        )\n",
    "\n",
    "\n",
    "column_converters[columns.get_index_of(run.run_history)] = parquet_to_dataframe\n",
    "column_converters[columns.get_index_of(run.run_extended_history)] = parquet_to_dataframe\n",
    "\n",
    "\n",
    "dfs = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data(query_search):\n",
    "    with ConnectionManager(credentials) as connection:\n",
    "        query = psycopg.sql.SQL(\n",
    "            \"\"\"\n",
    "    SELECT\n",
    "        {columns}\n",
    "    FROM\n",
    "        {run},\n",
    "        {job_status},\n",
    "        {job_data}\n",
    "    WHERE\n",
    "        {run}.batch LIKE {pattern}\n",
    "        AND {job_status}.id = {run}.run_id\n",
    "        AND {job_status}.id = {job_data}.id\n",
    "        AND {job_status}.status = 2\n",
    "        AND {job_data}.command @> {json_data}::jsonb\n",
    "    ORDER BY\n",
    "        experiment_id, run_id;\n",
    "    \"\"\"\n",
    "        ).format(\n",
    "            columns=columns.columns_sql,\n",
    "            run=run.identifier,\n",
    "            job_status=job_status.identifier,\n",
    "            job_data=job_data.identifier,\n",
    "            pattern=sql.Literal(\"%energy%\"),\n",
    "            json_data=sql.Literal(query_search),\n",
    "        )\n",
    "\n",
    "        with ClientCursor(connection) as c:\n",
    "            print(c.mogrify(query))\n",
    "\n",
    "        with connection.cursor(binary=True) as cursor:\n",
    "            cursor.execute(query, binary=True)\n",
    "\n",
    "            for row in cursor:\n",
    "                row_data = {}\n",
    "                for i, (column, column_converter) in enumerate(\n",
    "                    zip(columns, column_converters)\n",
    "                ):\n",
    "                    column_converter(row, i, row[i], column, row_data)\n",
    "\n",
    "                row_df = row_data[\"run_history\"]\n",
    "                row_df = row_df.join(\n",
    "                    row_data[\"run_extended_history\"], on=\"epoch\", how=\"left\", rsuffix=\"_\"\n",
    "                )\n",
    "                for k in (\"run_history\", \"run_extended_history\"):\n",
    "                    del row_data[k]\n",
    "\n",
    "                for k, v in row_data.items():\n",
    "                    if k in row_df:\n",
    "                        pass\n",
    "                    if isinstance(v, list):\n",
    "                        row_df[k] = [v] * len(row_df)\n",
    "                    else:\n",
    "                        row_df[k] = v\n",
    "                    # data.setdefault(k, []).append(v)\n",
    "                dfs.append(row_df)\n",
    "\n",
    "    data = pandas.concat(dfs)\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        for column in data.columns:\n",
    "            if isinstance(row[column], uuid.UUID):\n",
    "                data.at[index, column] = str(row[column])\n",
    "\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import tqdm as tqmd\n",
    "def save_data(queries:list):\n",
    "    for query in tqmd.tqdm(queries):\n",
    "        data = get_data(query)\n",
    "        # convert to pyarrow table\n",
    "        table = pa.Table.from_pandas(data)\n",
    "\n",
    "        # write to distributed parquet file saved as ['name','depth','size','shape']\n",
    "        pq.write_to_dataset(table, root_path='/projects/gcomp/jgafur/dataset/', partition_cols=['name','shape','depth','size'])\n",
    "        del data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm as tqmd\n",
    "from multiprocessing import Pool\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "\n",
    "def save_parallel(query):\n",
    "    data = get_data(query)\n",
    "    table = pa.Table.from_pandas(data)\n",
    "    pq.write_to_dataset(table, root_path='/projects/gcomp/jgafur/dataset/', partition_cols=['name', 'shape', 'depth', 'size'])\n",
    "    del data\n",
    "\n",
    "def save_data_parallelized(queries):\n",
    "    with Pool() as pool:\n",
    "        list(tqmd.tqdm(pool.imap(save_parallel, queries), total=len(queries)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def get_json(**kwargs):\n",
    "    # Given dictionary\n",
    "    given_dict = {}\n",
    "   \n",
    "    # Update the given dictionary with user input\n",
    "    given_dict.update(kwargs)\n",
    "\n",
    "    # Convert the updated dictionary to JSON\n",
    "    json_result = json.dumps(given_dict, indent=2)\n",
    "\n",
    "    return json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    SELECT\n",
      "        \"experiment_id\",\"run_timestamp\",\"run_id\",\"job_id\",\"seed\",\"slurm_job_id\",\"task_version\",\"num_nodes\",\"num_cpus\",\"num_gpus\",\"gpu_memory\",\"host_name\",\"batch\",\"run_data\",\"run_history\",\"run_extended_history\",\"queue\",\"status\",\"priority\",\"start_time\",\"update_time\",\"worker\",\"error_count\",\"error\",\"parent\",\"command\"\n",
      "    FROM\n",
      "        \"run\",\n",
      "        \"job_status\",\n",
      "        \"job_data\"\n",
      "    WHERE\n",
      "        \"run\".batch LIKE '%energy%'\n",
      "        AND \"job_status\".id = \"run\".run_id\n",
      "        AND \"job_status\".id = \"job_data\".id\n",
      "        AND \"job_status\".status = 2\n",
      "        AND \"job_data\".command @> '{\n",
      "  \"dataset\": {\n",
      "    \"name\": \"splice\"\n",
      "  }\n",
      "}'::jsonb\n",
      "    ORDER BY\n",
      "        experiment_id, run_id;\n",
      "    \n",
      "    SELECT\n",
      "        \"experiment_id\",\"run_timestamp\",\"run_id\",\"job_id\",\"seed\",\"slurm_job_id\",\"task_version\",\"num_nodes\",\"num_cpus\",\"num_gpus\",\"gpu_memory\",\"host_name\",\"batch\",\"run_data\",\"run_history\",\"run_extended_history\",\"queue\",\"status\",\"priority\",\"start_time\",\"update_time\",\"worker\",\"error_count\",\"error\",\"parent\",\"command\"\n",
      "    FROM\n",
      "        \"run\",\n",
      "        \"job_status\",\n",
      "        \"job_data\"\n",
      "    WHERE\n",
      "        \"run\".batch LIKE '%energy%'\n",
      "        AND \"job_status\".id = \"run\".run_id\n",
      "        AND \"job_status\".id = \"job_data\".id\n",
      "        AND \"job_status\".status = 2\n",
      "        AND \"job_data\".command @> '{\n",
      "  \"dataset\": {\n",
      "    \"name\": \"sleep\"\n",
      "  }\n",
      "}'::jsonb\n",
      "    ORDER BY\n",
      "        experiment_id, run_id;\n",
      "    \n",
      "    SELECT\n",
      "        \"experiment_id\",\"run_timestamp\",\"run_id\",\"job_id\",\"seed\",\"slurm_job_id\",\"task_version\",\"num_nodes\",\"num_cpus\",\"num_gpus\",\"gpu_memory\",\"host_name\",\"batch\",\"run_data\",\"run_history\",\"run_extended_history\",\"queue\",\"status\",\"priority\",\"start_time\",\"update_time\",\"worker\",\"error_count\",\"error\",\"parent\",\"command\"\n",
      "    FROM\n",
      "        \"run\",\n",
      "        \"job_status\",\n",
      "        \"job_data\"\n",
      "    WHERE\n",
      "        \"run\".batch LIKE '%energy%'\n",
      "        AND \"job_status\".id = \"run\".run_id\n",
      "        AND \"job_status\".id = \"job_data\".id\n",
      "        AND \"job_status\".status = 2\n",
      "        AND \"job_data\".command @> '{\n",
      "  \"dataset\": {\n",
      "    \"name\": \"529_pollen\"\n",
      "  }\n",
      "}'::jsonb\n",
      "    ORDER BY\n",
      "        experiment_id, run_id;\n",
      "    \n",
      "    SELECT\n",
      "        \"experiment_id\",\"run_timestamp\",\"run_id\",\"job_id\",\"seed\",\"slurm_job_id\",\"task_version\",\"num_nodes\",\"num_cpus\",\"num_gpus\",\"gpu_memory\",\"host_name\",\"batch\",\"run_data\",\"run_history\",\"run_extended_history\",\"queue\",\"status\",\"priority\",\"start_time\",\"update_time\",\"worker\",\"error_count\",\"error\",\"parent\",\"command\"\n",
      "    FROM\n",
      "        \"run\",\n",
      "        \"job_status\",\n",
      "        \"job_data\"\n",
      "    WHERE\n",
      "        \"run\".batch LIKE '%energy%'\n",
      "        AND \"job_status\".id = \"run\".run_id\n",
      "        AND \"job_status\".id = \"job_data\".id\n",
      "        AND \"job_status\".status = 2\n",
      "        AND \"job_data\".command @> '{\n",
      "  \"dataset\": {\n",
      "    \"name\": \"banana\"\n",
      "  }\n",
      "}'::jsonb\n",
      "    ORDER BY\n",
      "        experiment_id, run_id;\n",
      "    \n",
      "    SELECT\n",
      "        \"experiment_id\",\"run_timestamp\",\"run_id\",\"job_id\",\"seed\",\"slurm_job_id\",\"task_version\",\"num_nodes\",\"num_cpus\",\"num_gpus\",\"gpu_memory\",\"host_name\",\"batch\",\"run_data\",\"run_history\",\"run_extended_history\",\"queue\",\"status\",\"priority\",\"start_time\",\"update_time\",\"worker\",\"error_count\",\"error\",\"parent\",\"command\"\n",
      "    FROM\n",
      "        \"run\",\n",
      "        \"job_status\",\n",
      "        \"job_data\"\n",
      "    WHERE\n",
      "        \"run\".batch LIKE '%energy%'\n",
      "        AND \"job_status\".id = \"run\".run_id\n",
      "        AND \"job_status\".id = \"job_data\".id\n",
      "        AND \"job_status\".status = 2\n",
      "        AND \"job_data\".command @> '{\n",
      "  \"dataset\": {\n",
      "    \"name\": \"201_pol\"\n",
      "  }\n",
      "}'::jsonb\n",
      "    ORDER BY\n",
      "        experiment_id, run_id;\n",
      "    \n",
      "    SELECT\n",
      "        \"experiment_id\",\"run_timestamp\",\"run_id\",\"job_id\",\"seed\",\"slurm_job_id\",\"task_version\",\"num_nodes\",\"num_cpus\",\"num_gpus\",\"gpu_memory\",\"host_name\",\"batch\",\"run_data\",\"run_history\",\"run_extended_history\",\"queue\",\"status\",\"priority\",\"start_time\",\"update_time\",\"worker\",\"error_count\",\"error\",\"parent\",\"command\"\n",
      "    FROM\n",
      "        \"run\",\n",
      "        \"job_status\",\n",
      "        \"job_data\"\n",
      "    WHERE\n",
      "        \"run\".batch LIKE '%energy%'\n",
      "        AND \"job_status\".id = \"run\".run_id\n",
      "        AND \"job_status\".id = \"job_data\".id\n",
      "        AND \"job_status\".status = 2\n",
      "        AND \"job_data\".command @> '{\n",
      "  \"dataset\": {\n",
      "    \"name\": \"wine_quality_white\"\n",
      "  }\n",
      "}'::jsonb\n",
      "    ORDER BY\n",
      "        experiment_id, run_id;\n",
      "    \n",
      "    SELECT\n",
      "        \"experiment_id\",\"run_timestamp\",\"run_id\",\"job_id\",\"seed\",\"slurm_job_id\",\"task_version\",\"num_nodes\",\"num_cpus\",\"num_gpus\",\"gpu_memory\",\"host_name\",\"batch\",\"run_data\",\"run_history\",\"run_extended_history\",\"queue\",\"status\",\"priority\",\"start_time\",\"update_time\",\"worker\",\"error_count\",\"error\",\"parent\",\"command\"\n",
      "    FROM\n",
      "        \"run\",\n",
      "        \"job_status\",\n",
      "        \"job_data\"\n",
      "    WHERE\n",
      "        \"run\".batch LIKE '%energy%'\n",
      "        AND \"job_status\".id = \"run\".run_id\n",
      "        AND \"job_status\".id = \"job_data\".id\n",
      "        AND \"job_status\".status = 2\n",
      "        AND \"job_data\".command @> '{\n",
      "  \"dataset\": {\n",
      "    \"name\": \"mnist\"\n",
      "  }\n",
      "}'::jsonb\n",
      "    ORDER BY\n",
      "        experiment_id, run_id;\n",
      "    \n",
      "    SELECT\n",
      "        \"experiment_id\",\"run_timestamp\",\"run_id\",\"job_id\",\"seed\",\"slurm_job_id\",\"task_version\",\"num_nodes\",\"num_cpus\",\"num_gpus\",\"gpu_memory\",\"host_name\",\"batch\",\"run_data\",\"run_history\",\"run_extended_history\",\"queue\",\"status\",\"priority\",\"start_time\",\"update_time\",\"worker\",\"error_count\",\"error\",\"parent\",\"command\"\n",
      "    FROM\n",
      "        \"run\",\n",
      "        \"job_status\",\n",
      "        \"job_data\"\n",
      "    WHERE\n",
      "        \"run\".batch LIKE '%energy%'\n",
      "        AND \"job_status\".id = \"run\".run_id\n",
      "        AND \"job_status\".id = \"job_data\".id\n",
      "        AND \"job_status\".status = 2\n",
      "        AND \"job_data\".command @> '{\n",
      "  \"dataset\": {\n",
      "    \"name\": \"nursery\"\n",
      "  }\n",
      "}'::jsonb\n",
      "    ORDER BY\n",
      "        experiment_id, run_id;\n",
      "    \n",
      "    SELECT\n",
      "        \"experiment_id\",\"run_timestamp\",\"run_id\",\"job_id\",\"seed\",\"slurm_job_id\",\"task_version\",\"num_nodes\",\"num_cpus\",\"num_gpus\",\"gpu_memory\",\"host_name\",\"batch\",\"run_data\",\"run_history\",\"run_extended_history\",\"queue\",\"status\",\"priority\",\"start_time\",\"update_time\",\"worker\",\"error_count\",\"error\",\"parent\",\"command\"\n",
      "    FROM\n",
      "        \"run\",\n",
      "        \"job_status\",\n",
      "        \"job_data\"\n",
      "    WHERE\n",
      "        \"run\".batch LIKE '%energy%'\n",
      "        AND \"job_status\".id = \"run\".run_id\n",
      "        AND \"job_status\".id = \"job_data\".id\n",
      "        AND \"job_status\".status = 2\n",
      "        AND \"job_data\".command @> '{\n",
      "  \"dataset\": {\n",
      "    \"name\": \"connect_4\"\n",
      "  }\n",
      "}'::jsonb\n",
      "    ORDER BY\n",
      "        experiment_id, run_id;\n",
      "    \n",
      "\n",
      "    SELECT\n",
      "        \"experiment_id\",\"run_timestamp\",\"run_id\",\"job_id\",\"seed\",\"slurm_job_id\",\"task_version\",\"num_nodes\",\"num_cpus\",\"num_gpus\",\"gpu_memory\",\"host_name\",\"batch\",\"run_data\",\"run_history\",\"run_extended_history\",\"queue\",\"status\",\"priority\",\"start_time\",\"update_time\",\"worker\",\"error_count\",\"error\",\"parent\",\"command\"\n",
      "    FROM\n",
      "        \"run\",\n",
      "        \"job_status\",\n",
      "        \"job_data\"\n",
      "    WHERE\n",
      "        \"run\".batch LIKE '%energy%'\n",
      "        AND \"job_status\".id = \"run\".run_id\n",
      "        AND \"job_status\".id = \"job_data\".id\n",
      "        AND \"job_status\".status = 2\n",
      "        AND \"job_data\".command @> '{\n",
      "  \"dataset\": {\n",
      "    \"name\": \"537_houses\"\n",
      "  }\n",
      "}'::jsonb\n",
      "    ORDER BY\n",
      "        experiment_id, run_id;\n",
      "    \n",
      "    SELECT\n",
      "        \"experiment_id\",\"run_timestamp\",\"run_id\",\"job_id\",\"seed\",\"slurm_job_id\",\"task_version\",\"num_nodes\",\"num_cpus\",\"num_gpus\",\"gpu_memory\",\"host_name\",\"batch\",\"run_data\",\"run_history\",\"run_extended_history\",\"queue\",\"status\",\"priority\",\"start_time\",\"update_time\",\"worker\",\"error_count\",\"error\",\"parent\",\"command\"\n",
      "    FROM\n",
      "        \"run\",\n",
      "        \"job_status\",\n",
      "        \"job_data\"\n",
      "    WHERE\n",
      "        \"run\".batch LIKE '%energy%'\n",
      "        AND \"job_status\".id = \"run\".run_id\n",
      "        AND \"job_status\".id = \"job_data\".id\n",
      "        AND \"job_status\".status = 2\n",
      "        AND \"job_data\".command @> '{\n",
      "  \"dataset\": {\n",
      "    \"name\": \"294_satellite_image\"\n",
      "  }\n",
      "}'::jsonb\n",
      "    ORDER BY\n",
      "        experiment_id, run_id;\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    SELECT\n",
      "        \"experiment_id\",\"run_timestamp\",\"run_id\",\"job_id\",\"seed\",\"slurm_job_id\",\"task_version\",\"num_nodes\",\"num_cpus\",\"num_gpus\",\"gpu_memory\",\"host_name\",\"batch\",\"run_data\",\"run_history\",\"run_extended_history\",\"queue\",\"status\",\"priority\",\"start_time\",\"update_time\",\"worker\",\"error_count\",\"error\",\"parent\",\"command\"\n",
      "    FROM\n",
      "        \"run\",\n",
      "        \"job_status\",\n",
      "        \"job_data\"\n",
      "    WHERE\n",
      "        \"run\".batch LIKE '%energy%'\n",
      "        AND \"job_status\".id = \"run\".run_id\n",
      "        AND \"job_status\".id = \"job_data\".id\n",
      "        AND \"job_status\".status = 2\n",
      "        AND \"job_data\".command @> '{\n",
      "  \"dataset\": {\n",
      "    \"name\": \"adult\"\n",
      "  }\n",
      "}'::jsonb\n",
      "    ORDER BY\n",
      "        experiment_id, run_id;\n",
      "    \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "    SELECT\n",
      "        \"experiment_id\",\"run_timestamp\",\"run_id\",\"job_id\",\"seed\",\"slurm_job_id\",\"task_version\",\"num_nodes\",\"num_cpus\",\"num_gpus\",\"gpu_memory\",\"host_name\",\"batch\",\"run_data\",\"run_history\",\"run_extended_history\",\"queue\",\"status\",\"priority\",\"start_time\",\"update_time\",\"worker\",\"error_count\",\"error\",\"parent\",\"command\"\n",
      "    FROM\n",
      "        \"run\",\n",
      "        \"job_status\",\n",
      "        \"job_data\"\n",
      "    WHERE\n",
      "        \"run\".batch LIKE '%energy%'\n",
      "        AND \"job_status\".id = \"run\".run_id\n",
      "        AND \"job_status\".id = \"job_data\".id\n",
      "        AND \"job_status\".status = 2\n",
      "        AND \"job_data\".command @> '{\n",
      "  \"dataset\": {\n",
      "    \"name\": \"505_tecator\"\n",
      "  }\n",
      "}'::jsonb\n",
      "    ORDER BY\n",
      "        experiment_id, run_id;\n",
      "    \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13 [02:56<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/nopt/nrel/apps/anaconda/mini_4.9.2/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    852\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 853\u001b[0;31m                 \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    854\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: pop from an empty deque",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-1a63456fa591>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0msave_data_parallelized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery_search1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_search2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_search3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_search4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_search5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_search6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_search7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_search8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_search9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_search10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_search11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_search12\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery_search13\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-f47b6ed44765>\u001b[0m in \u001b[0;36msave_data_parallelized\u001b[0;34m(queries)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msave_data_parallelized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mPool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqmd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_parallel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/nopt/nrel/apps/anaconda/mini_4.9.2/lib/python3.9/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1179\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nopt/nrel/apps/anaconda/mini_4.9.2/lib/python3.9/multiprocessing/pool.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    856\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 858\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    859\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    860\u001b[0m                     \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_items\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpopleft\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/nopt/nrel/apps/anaconda/mini_4.9.2/lib/python3.9/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "query_search1 = get_json( dataset={\"name\": \"sleep\"}\n",
    "        )\n",
    "\n",
    "query_search2 = get_json(dataset={\"name\":\"banana\"},\n",
    "        )\n",
    "\n",
    "query_search3 = get_json(dataset={\"name\":\"connect_4\"},\n",
    "        )\n",
    "\n",
    "query_search4 = get_json(dataset={\"name\":\"mnist\"},\n",
    "        )\n",
    "\n",
    "query_search5 = get_json(dataset={\"name\":\"nursery\"},\n",
    "        )\n",
    "\n",
    "query_search6 = get_json(dataset={\"name\":\"splice\"},\n",
    "        )\n",
    "\n",
    "query_search7 = get_json(dataset={\"name\":\"wine_quality_white\"},\n",
    "        )\n",
    "\n",
    "query_search8 = get_json(dataset={\"name\":\"201_pol\"},\n",
    "        )\n",
    "\n",
    "query_search9 = get_json(dataset={\"name\":\"294_satellite_image\"},\n",
    "        )\n",
    "\n",
    "query_search10 = get_json(dataset={\"name\":\"505_tecator\"},\n",
    "        )\n",
    "\n",
    "query_search11 = get_json(dataset={\"name\":\"529_pollen\"},                 \n",
    "        )\n",
    "\n",
    "query_search12 = get_json(dataset={\"name\":\"537_houses\"},\n",
    "        )\n",
    "\n",
    "query_search13 = get_json(dataset={\"name\":\"adult\"},\n",
    "        )     \n",
    "\n",
    "\n",
    "save_data_parallelized([query_search1, query_search2, query_search3, query_search4, query_search5, query_search6, query_search7, query_search8, query_search9, query_search10, query_search11, query_search12, query_search13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
