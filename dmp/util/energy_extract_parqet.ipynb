{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 16:03:14.180828: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import io\n",
    "import uuid\n",
    "import psycopg.sql\n",
    "import pyarrow\n",
    "import pyarrow.parquet\n",
    "\n",
    "import jobqueue\n",
    "from jobqueue.connection_manager import ConnectionManager\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from typing import Callable, List\n",
    "\n",
    "from psycopg import sql\n",
    "\n",
    "import dmp.keras_interface.model_serialization as model_serialization\n",
    "from dmp.task.experiment.training_experiment.training_epoch import TrainingEpoch\n",
    "from dmp.postgres_interface.element.column import Column\n",
    "from dmp.postgres_interface.element.table import Table\n",
    "from dmp.postgres_interface.element.column_group import ColumnGroup\n",
    "\n",
    "from dmp.util.butter_e_export import *\n",
    "\n",
    "pd.options.display.max_seq_items = None\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "credentials = jobqueue.load_credentials(\"dmp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: convert this into two steps:\n",
    "    1) Get the ids of the runs we want to extract (in energy batches) ordered by experiment_id\n",
    "\n",
    "          SELECT\n",
    "                  {run}.{run_id}\n",
    "          FROM\n",
    "                  {run},\n",
    "                  {job_status},\n",
    "                  {job_data}\n",
    "          WHERE TRUE\n",
    "                  AND {run}.batch like {pattern}\n",
    "                  AND {job_status}.id = {run}.run_id\n",
    "                  AND {job_status}.id = {job_data}.id\n",
    "              AND {job_status}.status = 2\n",
    "          ORDER BY experiment_id, run_id;\n",
    "\n",
    "    2) In parallel (using the multiprocessing lib), extract blocks of ids into a partitioned parquet file\n",
    "      -> partition by the attributes we care about querying by (dataset, size, shape, depth)\n",
    "\n",
    "              SELECT\n",
    "                  {columns}\n",
    "              FROM\n",
    "                  {run},\n",
    "                  {job_status},\n",
    "                  {job_data}\n",
    "              WHERE TRUE\n",
    "                  AND {job_status}.id = {run}.run_id\n",
    "                  AND {job_status}.id = {job_data}.id\n",
    "                  AND {job_status}.status = 2\n",
    "                  AND {run}.{run_id} IN ({ids})\n",
    "\n",
    "        pool = multiprocessing.ProcessPool(multiprocessing.cpu_count())\n",
    "        results = pool.uimap(download_chunk, chunks)\n",
    "        for num_rows, chunk in results:\n",
    "            num_stored += 1\n",
    "            print(f\"Stored {num_rows} in chunk {chunk}, {num_stored} / {len(chunks)}.\")\n",
    "\n",
    "    extra credit) extract butter data matching this as well into a new dataset\n",
    "    extra credit) make a summary dataset that summarizes the quartiles of # epochs to reach target test loss levels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run vars {'_name': 'run', '_columns': (), '_index': None}\n",
      "('experiment_id', 'run_timestamp', 'run_id', 'job_id', 'seed', 'slurm_job_id', 'task_version', 'num_nodes', 'num_cpus', 'num_gpus', 'gpu_memory', 'host_name', 'batch', 'run_data', 'run_history', 'run_extended_history', 'queue', 'status', 'priority', 'start_time', 'update_time', 'worker', 'error_count', 'error', 'parent', 'command')\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n"
     ]
    }
   ],
   "source": [
    "from psycopg import ClientCursor\n",
    "\n",
    "\n",
    "print(f\"run vars {vars(run)}\")\n",
    "\n",
    "columns = (\n",
    "    run\n",
    "    + ColumnGroup(*[c for c in job_status.columns if c.name != \"id\"])\n",
    "    + job_data.command\n",
    ")\n",
    "print(columns.names)\n",
    "\n",
    "\n",
    "def passthrough(row, index, value, column, data):\n",
    "    data[column.name] = value\n",
    "\n",
    "\n",
    "column_converters: List[Callable] = [passthrough for _ in columns]\n",
    "\n",
    "\n",
    "def flatten_json(json_obj, destination=None, parent_key=\"\", separator=\"_\"):\n",
    "    if isinstance(destination, dict):\n",
    "        flattened = destination\n",
    "    else:\n",
    "        flattened = {}\n",
    "\n",
    "    for key, value in json_obj.items():\n",
    "        new_key = f\"{parent_key}{separator}{key}\" if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            flattened.update(flatten_json(value, new_key, separator=separator))\n",
    "        else:\n",
    "            flattened[new_key] = value\n",
    "    return flattened\n",
    "\n",
    "\n",
    "column_converters[\n",
    "    columns.get_index_of(job_data.command)\n",
    "] = lambda row, index, value, column, data: flatten_json(value, destination=data)\n",
    "column_converters[\n",
    "    columns.get_index_of(run.run_data)\n",
    "] = lambda row, index, value, column, data: flatten_json(value, destination=data)\n",
    "\n",
    "\n",
    "def parquet_to_dataframe(row, index, value, column, data):\n",
    "    with io.BytesIO(value) as buffer:\n",
    "        data[column.name] = (\n",
    "            pyarrow.parquet.read_table(pyarrow.PythonFile(buffer, mode=\"r\"))\n",
    "            .to_pandas()\n",
    "            .sort_values(by=\"epoch\")\n",
    "        )\n",
    "\n",
    "\n",
    "column_converters[columns.get_index_of(run.run_history)] = parquet_to_dataframe\n",
    "column_converters[columns.get_index_of(run.run_extended_history)] = parquet_to_dataframe\n",
    "\n",
    "\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import multiprocessing \n",
    "import tqdm\n",
    "\n",
    "def get_ids(**kwargs):\n",
    "    with ConnectionManager(credentials) as connection:\n",
    "        query = psycopg.sql.SQL(\n",
    "            \"\"\"\n",
    "                SELECT\n",
    "                    {run}.run_id\n",
    "                FROM\n",
    "                    {run},\n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "                WHERE TRUE\n",
    "                    AND {run}.batch like {pattern}\n",
    "                    AND {job_status}.id = {run}.run_id\n",
    "                    AND {job_status}.id = {job_data}.id\n",
    "                    AND {job_status}.status = 2\n",
    "                    AND {job_data}.command @> {json_data}::jsonb\n",
    "                ORDER BY experiment_id, run_id;\n",
    "            \"\"\" \n",
    "        ).format(\n",
    "            run=run.identifier,\n",
    "            job_status=job_status.identifier,\n",
    "            job_data=job_data.identifier,\n",
    "            pattern=sql.Literal(\"%energy%\"),\n",
    "            json_data=sql.Literal(get_json(**kwargs)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        with ClientCursor(connection) as c:\n",
    "           c.mogrify(query)\n",
    "\n",
    "        with connection.cursor(binary=True) as cursor:\n",
    "            cursor.execute(query, binary=True)\n",
    "            ids = cursor.fetchall()\n",
    "            rows = []\n",
    "            for id in ids:\n",
    "                rows.append(str(id[0]))\n",
    "            return rows\n",
    "        \n",
    "def get_json(**kwargs):\n",
    "    # Given dictionary\n",
    "    given_dict = {}\n",
    "   \n",
    "    # Update the given dictionary with user input\n",
    "    given_dict.update(kwargs)\n",
    "\n",
    "    # Convert the updated dictionary to JSON\n",
    "    json_result = json.dumps(given_dict, indent=2)\n",
    "\n",
    "    return json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(d):\n",
    "    keys = []\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            keys.extend(get_keys(v))\n",
    "        else:\n",
    "            keys.append((k, v))\n",
    "    return keys\n",
    "\n",
    "def save_data(run_ids):\n",
    "    with ConnectionManager(credentials) as connection:\n",
    "        query = psycopg.sql.SQL(\n",
    "            \"\"\"\n",
    "                SELECT\n",
    "                    {columns}\n",
    "                FROM\n",
    "                    {run}, \n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "                WHERE\n",
    "                \t{job_status}.id = {run}.run_id\n",
    "                    AND {job_status}.id = {job_data}.id\n",
    "                    AND {job_status}.status = 2\n",
    "                    AND {run}.run_id IN ({run_ids});\n",
    "            \"\"\" \n",
    "        ).format(\n",
    "            columns=columns.columns_sql,\n",
    "            run_ids=sql.SQL(', ').join(map(sql.Literal, run_ids)),\n",
    "            run=run.identifier,\n",
    "            job_data=job_data.identifier,\n",
    "            job_status=job_status.identifier,\n",
    "        )\n",
    "       \n",
    "        with ClientCursor(connection) as c:\n",
    "            c.mogrify(query)\n",
    "        \n",
    "        with connection.cursor(binary=True) as cursor:\n",
    "            cursor.execute(query, binary=True)\n",
    "            rows = cursor.fetchall()\n",
    "            \n",
    "        # cast rows to a dataframe\n",
    "        df = pd.DataFrame(rows, columns=columns.names)\n",
    "        \n",
    "        data = []\n",
    "        data_names = []\n",
    "        # iterate through the rows\n",
    "        for index, row in df.iterrows():\n",
    "            # get the keys from the command column\n",
    "            keys = get_keys(row['command'])\n",
    "            data_names = [key[0] for key in keys]\n",
    "            data.append([key[1] for key in keys])\n",
    "        \n",
    "        # create a dataframe from the data\n",
    "        df2 = pd.DataFrame(data, columns=data_names)\n",
    "        # get intersection of columns\n",
    "        intesection = list(set(df.columns) & set(df2.columns))\n",
    "        # drop the intersection from the first dataframe\n",
    "        df2 = df2.drop(intesection, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "        # merge the dataframes\n",
    "        df = pd.concat([df, df2], axis=1)\n",
    "\n",
    "        # convert to pyarrow table\n",
    "        table = pa.Table.from_pandas(df)\n",
    "\n",
    "        # write to distributed parquet file saved as ['name','depth','size','shape']\n",
    "        pq.write_to_dataset(table, root_path='.', partition_cols=['name','shape','size','depth'])\n",
    "        \n",
    "        return df # delete the dataframe to free up memory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_chunk_lists(lists, elements_per_chunk):\n",
    "    combined = []\n",
    "    for l in lists:\n",
    "        combined.extend(l)\n",
    "    # remove duplicates\n",
    "    combined = list(set(combined))\n",
    "    # split into chunks\n",
    "    chunks = len(combined) // elements_per_chunk\n",
    "    combined = np.array_split(combined, chunks)\n",
    "    return combined\n",
    "\n",
    "def parallel_save_data(chunks):\n",
    "    for chunk in tqdm.tqdm(chunks):\n",
    "        return save_data(chunk)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ids: 32\n",
      "Number of ids: 30\n",
      "Number of batches: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6 [00:02<?, ?it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>run_timestamp</th>\n",
       "      <th>run_id</th>\n",
       "      <th>job_id</th>\n",
       "      <th>seed</th>\n",
       "      <th>slurm_job_id</th>\n",
       "      <th>task_version</th>\n",
       "      <th>num_nodes</th>\n",
       "      <th>num_cpus</th>\n",
       "      <th>num_gpus</th>\n",
       "      <th>...</th>\n",
       "      <th>method</th>\n",
       "      <th>source</th>\n",
       "      <th>test_split</th>\n",
       "      <th>label_noise</th>\n",
       "      <th>validation_split</th>\n",
       "      <th>run_tags</th>\n",
       "      <th>class</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>precision</th>\n",
       "      <th>early_stopping</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7a6907b6-ffbc-65e3-800a-5a2c76080b1a</td>\n",
       "      <td>2023-02-09 04:20:08.403282+00:00</td>\n",
       "      <td>0b1ae00c-8891-429f-bd78-3ec403102d95</td>\n",
       "      <td>0b1ae00c-8891-429f-bd78-3ec403102d95</td>\n",
       "      <td>1664220176</td>\n",
       "      <td>10392621</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>shuffled_train_test_split</td>\n",
       "      <td>pmlb</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>float32</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>68111018-118a-6574-9b6f-5971db015b14</td>\n",
       "      <td>2023-02-09 05:23:55.316701+00:00</td>\n",
       "      <td>1b5e0585-d693-4931-8e41-9438f3285198</td>\n",
       "      <td>1b5e0585-d693-4931-8e41-9438f3285198</td>\n",
       "      <td>1663278490</td>\n",
       "      <td>10371433</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>shuffled_train_test_split</td>\n",
       "      <td>pmlb</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>float32</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d8e38f8a-cea6-f3f5-4f65-74b2691dc89a</td>\n",
       "      <td>2023-02-09 05:22:34.900706+00:00</td>\n",
       "      <td>40e564cd-58e9-46cb-a93d-e57ee34a22e6</td>\n",
       "      <td>40e564cd-58e9-46cb-a93d-e57ee34a22e6</td>\n",
       "      <td>1664218096</td>\n",
       "      <td>10343385</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>shuffled_train_test_split</td>\n",
       "      <td>pmlb</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>float32</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b46fa758-29f4-cab1-e32c-13fb90447326</td>\n",
       "      <td>2023-02-09 05:55:09.264404+00:00</td>\n",
       "      <td>46d86fca-1c78-4ff4-868d-5ecaabdd05d3</td>\n",
       "      <td>46d86fca-1c78-4ff4-868d-5ecaabdd05d3</td>\n",
       "      <td>1663278815</td>\n",
       "      <td>10373627</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>shuffled_train_test_split</td>\n",
       "      <td>pmlb</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>float32</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97ce666d-e4fa-3435-1377-36f0ba0c05c4</td>\n",
       "      <td>2023-02-09 05:39:10.793787+00:00</td>\n",
       "      <td>57a90cb7-fd69-4fbc-b74c-ec723d202f80</td>\n",
       "      <td>57a90cb7-fd69-4fbc-b74c-ec723d202f80</td>\n",
       "      <td>1664218681</td>\n",
       "      <td>10343233</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>shuffled_train_test_split</td>\n",
       "      <td>pmlb</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>float32</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>a71de6b2-b936-ddd5-5bb3-cf9add27fd5d</td>\n",
       "      <td>2023-02-09 04:10:56.494323+00:00</td>\n",
       "      <td>7bb1f18f-da22-4370-8cfe-751c83113822</td>\n",
       "      <td>7bb1f18f-da22-4370-8cfe-751c83113822</td>\n",
       "      <td>1663280180</td>\n",
       "      <td>10296451</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>shuffled_train_test_split</td>\n",
       "      <td>pmlb</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>float32</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dfb84086-605b-20e0-af97-c753b71a4501</td>\n",
       "      <td>2023-02-09 04:11:35.690654+00:00</td>\n",
       "      <td>809ecbd3-e78d-4190-a4ec-ee1c5a2627b8</td>\n",
       "      <td>809ecbd3-e78d-4190-a4ec-ee1c5a2627b8</td>\n",
       "      <td>1664219981</td>\n",
       "      <td>10343277</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>shuffled_train_test_split</td>\n",
       "      <td>pmlb</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>float32</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>379de7a7-b29a-8d38-acb6-0b4e9a4f135b</td>\n",
       "      <td>2023-02-09 05:15:54.907863+00:00</td>\n",
       "      <td>8472b423-9e1a-4d1a-9d87-14d6154c3016</td>\n",
       "      <td>8472b423-9e1a-4d1a-9d87-14d6154c3016</td>\n",
       "      <td>1663279270</td>\n",
       "      <td>10296466</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>shuffled_train_test_split</td>\n",
       "      <td>pmlb</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>float32</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b0d3321c-59da-5494-8f5f-ce63d204b693</td>\n",
       "      <td>2023-02-09 04:10:31.210916+00:00</td>\n",
       "      <td>aa6ae69b-706d-4ada-99ec-4acb1c200e85</td>\n",
       "      <td>aa6ae69b-706d-4ada-99ec-4acb1c200e85</td>\n",
       "      <td>1664219461</td>\n",
       "      <td>10343126</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>shuffled_train_test_split</td>\n",
       "      <td>pmlb</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>float32</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ab7ed21b-7897-e4d4-fd00-93bbd3085273</td>\n",
       "      <td>2023-02-09 04:10:29.002482+00:00</td>\n",
       "      <td>c411140c-ae39-413c-88da-742cac2714cb</td>\n",
       "      <td>c411140c-ae39-413c-88da-742cac2714cb</td>\n",
       "      <td>1664219396</td>\n",
       "      <td>10343251</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>shuffled_train_test_split</td>\n",
       "      <td>pmlb</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>float32</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7a6907b6-ffbc-65e3-800a-5a2c76080b1a</td>\n",
       "      <td>2023-02-09 04:20:08.403282+00:00</td>\n",
       "      <td>df1e243e-7296-4c05-b2b1-b9240f90c34c</td>\n",
       "      <td>df1e243e-7296-4c05-b2b1-b9240f90c34c</td>\n",
       "      <td>1663280635</td>\n",
       "      <td>10371445</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>shuffled_train_test_split</td>\n",
       "      <td>pmlb</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>Adam</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>float32</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 65 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           experiment_id                    run_timestamp  \\\n",
       "0   7a6907b6-ffbc-65e3-800a-5a2c76080b1a 2023-02-09 04:20:08.403282+00:00   \n",
       "1   68111018-118a-6574-9b6f-5971db015b14 2023-02-09 05:23:55.316701+00:00   \n",
       "2   d8e38f8a-cea6-f3f5-4f65-74b2691dc89a 2023-02-09 05:22:34.900706+00:00   \n",
       "3   b46fa758-29f4-cab1-e32c-13fb90447326 2023-02-09 05:55:09.264404+00:00   \n",
       "4   97ce666d-e4fa-3435-1377-36f0ba0c05c4 2023-02-09 05:39:10.793787+00:00   \n",
       "5   a71de6b2-b936-ddd5-5bb3-cf9add27fd5d 2023-02-09 04:10:56.494323+00:00   \n",
       "6   dfb84086-605b-20e0-af97-c753b71a4501 2023-02-09 04:11:35.690654+00:00   \n",
       "7   379de7a7-b29a-8d38-acb6-0b4e9a4f135b 2023-02-09 05:15:54.907863+00:00   \n",
       "8   b0d3321c-59da-5494-8f5f-ce63d204b693 2023-02-09 04:10:31.210916+00:00   \n",
       "9   ab7ed21b-7897-e4d4-fd00-93bbd3085273 2023-02-09 04:10:29.002482+00:00   \n",
       "10  7a6907b6-ffbc-65e3-800a-5a2c76080b1a 2023-02-09 04:20:08.403282+00:00   \n",
       "\n",
       "                                  run_id  \\\n",
       "0   0b1ae00c-8891-429f-bd78-3ec403102d95   \n",
       "1   1b5e0585-d693-4931-8e41-9438f3285198   \n",
       "2   40e564cd-58e9-46cb-a93d-e57ee34a22e6   \n",
       "3   46d86fca-1c78-4ff4-868d-5ecaabdd05d3   \n",
       "4   57a90cb7-fd69-4fbc-b74c-ec723d202f80   \n",
       "5   7bb1f18f-da22-4370-8cfe-751c83113822   \n",
       "6   809ecbd3-e78d-4190-a4ec-ee1c5a2627b8   \n",
       "7   8472b423-9e1a-4d1a-9d87-14d6154c3016   \n",
       "8   aa6ae69b-706d-4ada-99ec-4acb1c200e85   \n",
       "9   c411140c-ae39-413c-88da-742cac2714cb   \n",
       "10  df1e243e-7296-4c05-b2b1-b9240f90c34c   \n",
       "\n",
       "                                  job_id        seed  slurm_job_id  \\\n",
       "0   0b1ae00c-8891-429f-bd78-3ec403102d95  1664220176      10392621   \n",
       "1   1b5e0585-d693-4931-8e41-9438f3285198  1663278490      10371433   \n",
       "2   40e564cd-58e9-46cb-a93d-e57ee34a22e6  1664218096      10343385   \n",
       "3   46d86fca-1c78-4ff4-868d-5ecaabdd05d3  1663278815      10373627   \n",
       "4   57a90cb7-fd69-4fbc-b74c-ec723d202f80  1664218681      10343233   \n",
       "5   7bb1f18f-da22-4370-8cfe-751c83113822  1663280180      10296451   \n",
       "6   809ecbd3-e78d-4190-a4ec-ee1c5a2627b8  1664219981      10343277   \n",
       "7   8472b423-9e1a-4d1a-9d87-14d6154c3016  1663279270      10296466   \n",
       "8   aa6ae69b-706d-4ada-99ec-4acb1c200e85  1664219461      10343126   \n",
       "9   c411140c-ae39-413c-88da-742cac2714cb  1664219396      10343251   \n",
       "10  df1e243e-7296-4c05-b2b1-b9240f90c34c  1663280635      10371445   \n",
       "\n",
       "    task_version  num_nodes  num_cpus  num_gpus  ...  \\\n",
       "0              3          2         2         0  ...   \n",
       "1              3          1         2         2  ...   \n",
       "2              3          2         2         0  ...   \n",
       "3              3          1         2         2  ...   \n",
       "4              3          2         2         0  ...   \n",
       "5              3          1         2         2  ...   \n",
       "6              3          2         2         0  ...   \n",
       "7              3          1         2         2  ...   \n",
       "8              3          2         2         0  ...   \n",
       "9              3          2         2         0  ...   \n",
       "10             3          1         2         2  ...   \n",
       "\n",
       "                       method source test_split label_noise validation_split  \\\n",
       "0   shuffled_train_test_split   pmlb        0.2         0.0              0.0   \n",
       "1   shuffled_train_test_split   pmlb        0.2         0.0              0.0   \n",
       "2   shuffled_train_test_split   pmlb        0.2         0.0              0.0   \n",
       "3   shuffled_train_test_split   pmlb        0.2         0.0              0.0   \n",
       "4   shuffled_train_test_split   pmlb        0.2         0.0              0.0   \n",
       "5   shuffled_train_test_split   pmlb        0.2         0.0              0.0   \n",
       "6   shuffled_train_test_split   pmlb        0.2         0.0              0.0   \n",
       "7   shuffled_train_test_split   pmlb        0.2         0.0              0.0   \n",
       "8   shuffled_train_test_split   pmlb        0.2         0.0              0.0   \n",
       "9   shuffled_train_test_split   pmlb        0.2         0.0              0.0   \n",
       "10  shuffled_train_test_split   pmlb        0.2         0.0              0.0   \n",
       "\n",
       "   run_tags  class  learning_rate  precision early_stopping  \n",
       "0      None   Adam         0.0001    float32           None  \n",
       "1      None   Adam         0.0001    float32           None  \n",
       "2      None   Adam         0.0001    float32           None  \n",
       "3      None   Adam         0.0001    float32           None  \n",
       "4      None   Adam         0.0001    float32           None  \n",
       "5      None   Adam         0.0001    float32           None  \n",
       "6      None   Adam         0.0001    float32           None  \n",
       "7      None   Adam         0.0001    float32           None  \n",
       "8      None   Adam         0.0001    float32           None  \n",
       "9      None   Adam         0.0001    float32           None  \n",
       "10     None   Adam         0.0001    float32           None  \n",
       "\n",
       "[11 rows x 65 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "#  get some example data\n",
    "ids_rec_depth5_sleep = get_ids(model={\"shape\":\"rectangle\", \"depth\":5},\n",
    "                dataset={\"name\":\"sleep\"}\n",
    "        )\n",
    "\n",
    "ids_rec_depth6_sleep = get_ids(model={\"shape\":\"rectangle\", \"depth\":6},\n",
    "                dataset={\"name\":\"sleep\"}\n",
    "        )\n",
    "\n",
    "print(f\"Number of ids: {len(ids_rec_depth5_sleep)}\")\n",
    "print(f\"Number of ids: {len(ids_rec_depth6_sleep)}\")\n",
    "\n",
    "batches = combine_chunk_lists([ids_rec_depth5_sleep, ids_rec_depth6_sleep], 10)\n",
    "\n",
    "print(f\"Number of batches: {len(batches)}\")\n",
    "\n",
    "parallel_save_data(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
