{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-20 16:50:52.248194: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import io\n",
    "import uuid\n",
    "import psycopg.sql\n",
    "import pyarrow\n",
    "import pyarrow.parquet\n",
    "\n",
    "import jobqueue\n",
    "from jobqueue.connection_manager import ConnectionManager\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from typing import Callable, List\n",
    "\n",
    "from psycopg import sql\n",
    "\n",
    "import dmp.keras_interface.model_serialization as model_serialization\n",
    "from dmp.task.experiment.training_experiment.training_epoch import TrainingEpoch\n",
    "from dmp.postgres_interface.element.column import Column\n",
    "from dmp.postgres_interface.element.table import Table\n",
    "from dmp.postgres_interface.element.column_group import ColumnGroup\n",
    "\n",
    "from dmp.util.butter_e_export import *\n",
    "\n",
    "pd.options.display.max_seq_items = None\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "credentials = jobqueue.load_credentials(\"dmp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: convert this into two steps:\n",
    "    1) Get the ids of the runs we want to extract (in energy batches) ordered by experiment_id\n",
    "\n",
    "          SELECT\n",
    "                  {run}.{run_id}\n",
    "          FROM\n",
    "                  {run},\n",
    "                  {job_status},\n",
    "                  {job_data}\n",
    "          WHERE TRUE\n",
    "                  AND {run}.batch like {pattern}\n",
    "                  AND {job_status}.id = {run}.run_id\n",
    "                  AND {job_status}.id = {job_data}.id\n",
    "              AND {job_status}.status = 2\n",
    "          ORDER BY experiment_id, run_id;\n",
    "\n",
    "    2) In parallel (using the multiprocessing lib), extract blocks of ids into a partitioned parquet file\n",
    "      -> partition by the attributes we care about querying by (dataset, size, shape, depth)\n",
    "\n",
    "              SELECT\n",
    "                  {columns}\n",
    "              FROM\n",
    "                  {run},\n",
    "                  {job_status},\n",
    "                  {job_data}\n",
    "              WHERE TRUE\n",
    "                  AND {job_status}.id = {run}.run_id\n",
    "                  AND {job_status}.id = {job_data}.id\n",
    "                  AND {job_status}.status = 2\n",
    "                  AND {run}.{run_id} IN ({ids})\n",
    "\n",
    "        pool = multiprocessing.ProcessPool(multiprocessing.cpu_count())\n",
    "        results = pool.uimap(download_chunk, chunks)\n",
    "        for num_rows, chunk in results:\n",
    "            num_stored += 1\n",
    "            print(f\"Stored {num_rows} in chunk {chunk}, {num_stored} / {len(chunks)}.\")\n",
    "\n",
    "    extra credit) extract butter data matching this as well into a new dataset\n",
    "    extra credit) make a summary dataset that summarizes the quartiles of # epochs to reach target test loss levels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run vars {'_name': 'run', '_columns': (), '_index': None}\n",
      "('experiment_id', 'run_timestamp', 'run_id', 'job_id', 'seed', 'slurm_job_id', 'task_version', 'num_nodes', 'num_cpus', 'num_gpus', 'gpu_memory', 'host_name', 'batch', 'run_data', 'run_history', 'run_extended_history', 'queue', 'status', 'priority', 'start_time', 'update_time', 'worker', 'error_count', 'error', 'parent', 'command')\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n"
     ]
    }
   ],
   "source": [
    "from psycopg import ClientCursor\n",
    "\n",
    "\n",
    "print(f\"run vars {vars(run)}\")\n",
    "\n",
    "columns = (\n",
    "    run\n",
    "    + ColumnGroup(*[c for c in job_status.columns if c.name != \"id\"])\n",
    "    + job_data.command\n",
    ")\n",
    "print(columns.names)\n",
    "\n",
    "\n",
    "def passthrough(row, index, value, column, data):\n",
    "    data[column.name] = value\n",
    "\n",
    "\n",
    "column_converters: List[Callable] = [passthrough for _ in columns]\n",
    "\n",
    "\n",
    "def flatten_json(json_obj, destination=None, parent_key=\"\", separator=\"_\"):\n",
    "    if isinstance(destination, dict):\n",
    "        flattened = destination\n",
    "    else:\n",
    "        flattened = {}\n",
    "\n",
    "    for key, value in json_obj.items():\n",
    "        new_key = f\"{parent_key}{separator}{key}\" if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            flattened.update(flatten_json(value, new_key, separator=separator))\n",
    "        else:\n",
    "            flattened[new_key] = value\n",
    "    return flattened\n",
    "\n",
    "\n",
    "column_converters[\n",
    "    columns.get_index_of(job_data.command)\n",
    "] = lambda row, index, value, column, data: flatten_json(value, destination=data)\n",
    "column_converters[\n",
    "    columns.get_index_of(run.run_data)\n",
    "] = lambda row, index, value, column, data: flatten_json(value, destination=data)\n",
    "\n",
    "\n",
    "def parquet_to_dataframe(row, index, value, column, data):\n",
    "    with io.BytesIO(value) as buffer:\n",
    "        data[column.name] = (\n",
    "            pyarrow.parquet.read_table(pyarrow.PythonFile(buffer, mode=\"r\"))\n",
    "            .to_pandas()\n",
    "            .sort_values(by=\"epoch\")\n",
    "        )\n",
    "\n",
    "\n",
    "column_converters[columns.get_index_of(run.run_history)] = parquet_to_dataframe\n",
    "column_converters[columns.get_index_of(run.run_extended_history)] = parquet_to_dataframe\n",
    "\n",
    "\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import multiprocessing \n",
    "import tqdm\n",
    "\n",
    "def get_ids(**kwargs):\n",
    "    with ConnectionManager(credentials) as connection:\n",
    "        query = psycopg.sql.SQL(\n",
    "            \"\"\"\n",
    "                SELECT\n",
    "                    {run}.run_id\n",
    "                FROM\n",
    "                    {run},\n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "                WHERE TRUE\n",
    "                    AND {run}.batch like {pattern}\n",
    "                    AND {job_status}.id = {run}.run_id\n",
    "                    AND {job_status}.id = {job_data}.id\n",
    "                    AND {job_status}.status = 2\n",
    "                    AND {job_data}.command @> {json_data}::jsonb\n",
    "                ORDER BY experiment_id, run_id;\n",
    "            \"\"\" \n",
    "        ).format(\n",
    "            run=run.identifier,\n",
    "            job_status=job_status.identifier,\n",
    "            job_data=job_data.identifier,\n",
    "            pattern=sql.Literal(\"%energy%\"),\n",
    "            json_data=sql.Literal(get_json(**kwargs)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        with ClientCursor(connection) as c:\n",
    "           c.mogrify(query)\n",
    "\n",
    "        with connection.cursor(binary=True) as cursor:\n",
    "            cursor.execute(query, binary=True)\n",
    "            ids = cursor.fetchall()\n",
    "            rows = []\n",
    "            for id in ids:\n",
    "                rows.append(str(id[0]))\n",
    "            return rows\n",
    "        \n",
    "def get_json(**kwargs):\n",
    "    # Given dictionary\n",
    "    given_dict = {}\n",
    "   \n",
    "    # Update the given dictionary with user input\n",
    "    given_dict.update(kwargs)\n",
    "\n",
    "    # Convert the updated dictionary to JSON\n",
    "    json_result = json.dumps(given_dict, indent=2)\n",
    "\n",
    "    return json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keys(d):\n",
    "    keys = []\n",
    "    for k, v in d.items():\n",
    "        if isinstance(v, dict):\n",
    "            keys.extend(get_keys(v))\n",
    "        else:\n",
    "            keys.append((k, v))\n",
    "    return keys\n",
    "\n",
    "def convert_bytes_to_dataframe(bytearray):\n",
    "    # data = None\n",
    "    # with io.BytesIO(bytearray) as buffer:\n",
    "    #     data = (\n",
    "    #         pyarrow.parquet.read_table(pyarrow.PythonFile(buffer, mode=\"r\"))\n",
    "    #         .to_pandas()\n",
    "    #         .sort_values(by=\"epoch\")\n",
    "    #     )\n",
    "    return str(bytearray)\n",
    "\n",
    "def save_data(run_ids):\n",
    "    with ConnectionManager(credentials) as connection:\n",
    "        query = psycopg.sql.SQL(\n",
    "            \"\"\"\n",
    "                SELECT\n",
    "                    {columns}\n",
    "                FROM\n",
    "                    {run}, \n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "                WHERE\n",
    "                \t{job_status}.id = {run}.run_id\n",
    "                    AND {job_status}.id = {job_data}.id\n",
    "                    AND {job_status}.status = 2\n",
    "                    AND {run}.run_id IN ({run_ids});\n",
    "            \"\"\" \n",
    "        ).format(\n",
    "            columns=columns.columns_sql,\n",
    "            run_ids=sql.SQL(', ').join(map(sql.Literal, run_ids)),\n",
    "            run=run.identifier,\n",
    "            job_data=job_data.identifier,\n",
    "            job_status=job_status.identifier,\n",
    "        )\n",
    "       \n",
    "        with ClientCursor(connection) as c:\n",
    "            c.mogrify(query)\n",
    "        \n",
    "        with connection.cursor(binary=True) as cursor:\n",
    "            cursor.execute(query, binary=True)\n",
    "            rows = cursor.fetchall()\n",
    "            \n",
    "        # cast rows to a dataframe\n",
    "        df = pd.DataFrame(rows, columns=columns.names)\n",
    "        \n",
    "        data = []\n",
    "        data_names = []\n",
    "        # iterate through the rows\n",
    "        for index, row in df.iterrows():\n",
    "            # get the keys from the command column\n",
    "            keys = get_keys(row['command'])\n",
    "            data_names = [key[0] for key in keys]\n",
    "            data.append([key[1] for key in keys])\n",
    "\n",
    "        # convert run_history and run_extended_history to dataframes\n",
    "        df['run_history'] = df['run_history'].apply(convert_bytes_to_dataframe)\n",
    "        df['run_extended_history'] = df['run_extended_history'].apply(convert_bytes_to_dataframe)\n",
    "        \n",
    "\n",
    "        # create a dataframe from the data\n",
    "        df2 = pd.DataFrame(data, columns=data_names)\n",
    "        # get intersection of columns\n",
    "        intesection = list(set(df.columns) & set(df2.columns))\n",
    "        # drop the intersection from the first dataframe\n",
    "        df2 = df2.drop(intesection, axis=1)\n",
    "\n",
    "\n",
    "        # merge the dataframes\n",
    "        df = pd.concat([df, df2], axis=1)\n",
    "\n",
    "        # for each row in the dataframe if the column is a UUID convert it to a string\n",
    "        for index, row in df.iterrows():\n",
    "            for column in df.columns:\n",
    "                if isinstance(row[column], uuid.UUID):\n",
    "                    df.at[index, column] = str(row[column])\n",
    "        \n",
    "        # drop duplicate columns\n",
    "        df = df.loc[:,~df.columns.duplicated()]\n",
    "\n",
    "        # convert to pyarrow table\n",
    "        table = pa.Table.from_pandas(df)\n",
    "\n",
    "        # write to distributed parquet file saved as ['name','depth','size','shape']\n",
    "        pq.write_to_dataset(table, root_path='.', partition_cols=['name','shape','depth','size'])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_chunk_lists(lists, elements_per_chunk):\n",
    "    combined = []\n",
    "    for l in lists:\n",
    "        combined.extend(l)\n",
    "    # remove duplicates\n",
    "    combined = list(set(combined))\n",
    "    # split into chunks\n",
    "    chunks = len(combined) // elements_per_chunk\n",
    "    combined = np.array_split(combined, chunks)\n",
    "    return combined\n",
    "\n",
    "def parallel_save_data(chunks):\n",
    "    # TODO: use multiprocessing to parallelize saving the data\n",
    "    for chunk in tqdm.tqdm(chunks):\n",
    "        save_data(chunk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of ids: 32\n",
      "Number of ids: 30\n",
      "Number of batches: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:06<00:00,  3.05s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#  get some example data\n",
    "ids_rec_depth5_sleep = get_ids(model={\"shape\":\"rectangle\", \"depth\":5},\n",
    "                dataset={\"name\":\"sleep\"}\n",
    "        )\n",
    "\n",
    "ids_rec_depth6_sleep = get_ids(model={\"shape\":\"rectangle\", \"depth\":6},\n",
    "                dataset={\"name\":\"sleep\"}\n",
    "        )\n",
    "\n",
    "print(f\"Number of ids: {len(ids_rec_depth5_sleep)}\")\n",
    "print(f\"Number of ids: {len(ids_rec_depth6_sleep)}\")\n",
    "\n",
    "batches = combine_chunk_lists([ids_rec_depth5_sleep, ids_rec_depth6_sleep], 30)\n",
    "\n",
    "print(f\"Number of batches: {len(batches)}\")\n",
    "\n",
    "parallel_save_data(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
