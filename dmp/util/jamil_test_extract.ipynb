{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 07:52:48.842089: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import io\n",
    "import uuid\n",
    "import psycopg.sql\n",
    "import pyarrow\n",
    "import pyarrow.parquet\n",
    "\n",
    "import jobqueue\n",
    "from jobqueue.connection_manager import ConnectionManager\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from typing import Callable, List\n",
    "\n",
    "from psycopg import sql\n",
    "\n",
    "import dmp.keras_interface.model_serialization as model_serialization\n",
    "from dmp.task.experiment.training_experiment.training_epoch import TrainingEpoch\n",
    "from dmp.postgres_interface.element.column import Column\n",
    "from dmp.postgres_interface.element.table import Table\n",
    "from dmp.postgres_interface.element.column_group import ColumnGroup\n",
    "\n",
    "from dmp.util.butter_e_export import *\n",
    "\n",
    "pd.options.display.max_seq_items = None\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "credentials = jobqueue.load_credentials(\"dmp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: convert this into two steps:\n",
    "    1) Get the ids of the runs we want to extract (in energy batches) ordered by experiment_id\n",
    "\n",
    "          SELECT\n",
    "                  {run}.{run_id}\n",
    "          FROM\n",
    "                  {run},\n",
    "                  {job_status},\n",
    "                  {job_data}\n",
    "          WHERE TRUE\n",
    "                  AND {run}.batch like {pattern}\n",
    "                  AND {job_status}.id = {run}.run_id\n",
    "                  AND {job_status}.id = {job_data}.id\n",
    "              AND {job_status}.status = 2\n",
    "          ORDER BY experiment_id, run_id;\n",
    "\n",
    "    2) In parallel (using the multiprocessing lib), extract blocks of ids into a partitioned parquet file\n",
    "      -> partition by the attributes we care about querying by (dataset, size, shape, depth)\n",
    "\n",
    "              SELECT\n",
    "                  {columns}\n",
    "              FROM\n",
    "                  {run},\n",
    "                  {job_status},\n",
    "                  {job_data}\n",
    "              WHERE TRUE\n",
    "                  AND {job_status}.id = {run}.run_id\n",
    "                  AND {job_status}.id = {job_data}.id\n",
    "                  AND {job_status}.status = 2\n",
    "                  AND {run}.{run_id} IN ({ids})\n",
    "\n",
    "        pool = multiprocessing.ProcessPool(multiprocessing.cpu_count())\n",
    "        results = pool.uimap(download_chunk, chunks)\n",
    "        for num_rows, chunk in results:\n",
    "            num_stored += 1\n",
    "            print(f\"Stored {num_rows} in chunk {chunk}, {num_stored} / {len(chunks)}.\")\n",
    "\n",
    "    extra credit) extract butter data matching this as well into a new dataset\n",
    "    extra credit) make a summary dataset that summarizes the quartiles of # epochs to reach target test loss levels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run vars {'_name': 'run', '_columns': (), '_index': None}\n",
      "('experiment_id', 'run_timestamp', 'run_id', 'job_id', 'seed', 'slurm_job_id', 'task_version', 'num_nodes', 'num_cpus', 'num_gpus', 'gpu_memory', 'host_name', 'batch', 'run_data', 'run_history', 'run_extended_history', 'queue', 'status', 'priority', 'start_time', 'update_time', 'worker', 'error_count', 'error', 'parent', 'command')\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n"
     ]
    }
   ],
   "source": [
    "from psycopg import ClientCursor\n",
    "\n",
    "\n",
    "print(f\"run vars {vars(run)}\")\n",
    "\n",
    "columns = (\n",
    "    run\n",
    "    + ColumnGroup(*[c for c in job_status.columns if c.name != \"id\"])\n",
    "    + job_data.command\n",
    ")\n",
    "print(columns.names)\n",
    "\n",
    "\n",
    "def passthrough(row, index, value, column, data):\n",
    "    data[column.name] = value\n",
    "\n",
    "\n",
    "column_converters: List[Callable] = [passthrough for _ in columns]\n",
    "\n",
    "\n",
    "def flatten_json(json_obj, destination=None, parent_key=\"\", separator=\"_\"):\n",
    "    if isinstance(destination, dict):\n",
    "        flattened = destination\n",
    "    else:\n",
    "        flattened = {}\n",
    "\n",
    "    for key, value in json_obj.items():\n",
    "        new_key = f\"{parent_key}{separator}{key}\" if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            flattened.update(flatten_json(value, new_key, separator=separator))\n",
    "        else:\n",
    "            flattened[new_key] = value\n",
    "    return flattened\n",
    "\n",
    "\n",
    "column_converters[\n",
    "    columns.get_index_of(job_data.command)\n",
    "] = lambda row, index, value, column, data: flatten_json(value, destination=data)\n",
    "column_converters[\n",
    "    columns.get_index_of(run.run_data)\n",
    "] = lambda row, index, value, column, data: flatten_json(value, destination=data)\n",
    "\n",
    "\n",
    "def parquet_to_dataframe(row, index, value, column, data):\n",
    "    with io.BytesIO(value) as buffer:\n",
    "        data[column.name] = (\n",
    "            pyarrow.parquet.read_table(pyarrow.PythonFile(buffer, mode=\"r\"))\n",
    "            .to_pandas()\n",
    "            .sort_values(by=\"epoch\")\n",
    "        )\n",
    "\n",
    "\n",
    "column_converters[columns.get_index_of(run.run_history)] = parquet_to_dataframe\n",
    "column_converters[columns.get_index_of(run.run_extended_history)] = parquet_to_dataframe\n",
    "\n",
    "\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                SELECT\n",
      "                    \"run\".run_id\n",
      "                FROM\n",
      "                    \"run\",\n",
      "                    \"job_status\",\n",
      "                    \"job_data\"\n",
      "                WHERE TRUE\n",
      "                    AND \"run\".batch like '%energy%'\n",
      "                    AND \"job_status\".id = \"run\".run_id\n",
      "                    AND \"job_status\".id = \"job_data\".id\n",
      "                    AND \"job_status\".status = 2\n",
      "                ORDER BY experiment_id, run_id\n",
      "                LIMIT 10;\n",
      "            \n"
     ]
    }
   ],
   "source": [
    "def get_ids():\n",
    "    with ConnectionManager(credentials) as connection:\n",
    "        \"\"\"\n",
    "        TODO: convert this into two steps:\n",
    "        1) Get the ids of the runs we want to extract (in energy batches) ordered by experiment_id\n",
    "\n",
    "            SELECT\n",
    "                    {run}.{run_id}\n",
    "            FROM\n",
    "                    {run},\n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "            WHERE TRUE\n",
    "                    AND {run}.batch like {pattern}\n",
    "                    AND {job_status}.id = {run}.run_id\n",
    "                    AND {job_status}.id = {job_data}.id\n",
    "                AND {job_status}.status = 2\n",
    "            ORDER BY experiment_id, run_id;\n",
    "\n",
    "        2) In parallel (using the multiprocessing lib), extract blocks of ids into a partitioned parquet file\n",
    "        -> partition by the attributes we care about querying by (dataset, size, shape, depth)\n",
    "\n",
    "                SELECT\n",
    "                    {columns}\n",
    "                FROM\n",
    "                    {run},\n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "                WHERE TRUE\n",
    "                    AND {job_status}.id = {run}.run_id\n",
    "                    AND {job_status}.id = {job_data}.id\n",
    "                    AND {job_status}.status = 2\n",
    "                    AND {run}.{run_id} IN ({ids})\n",
    "\n",
    "            pool = multiprocessing.ProcessPool(multiprocessing.cpu_count())\n",
    "            results = pool.uimap(download_chunk, chunks)\n",
    "            for num_rows, chunk in results:\n",
    "                num_stored += 1\n",
    "                print(f\"Stored {num_rows} in chunk {chunk}, {num_stored} / {len(chunks)}.\")\n",
    "\n",
    "        extra credit) extract butter data matching this as well into a new dataset\n",
    "        extra credit) make a summary dataset that summarizes the quartiles of # epochs to reach target test loss levels\n",
    "        \"\"\"\n",
    "        query = psycopg.sql.SQL(\n",
    "            \"\"\"\n",
    "                SELECT\n",
    "                    {run}.run_id\n",
    "                FROM\n",
    "                    {run},\n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "                WHERE TRUE\n",
    "                    AND {run}.batch like {pattern}\n",
    "                    AND {job_status}.id = {run}.run_id\n",
    "                    AND {job_status}.id = {job_data}.id\n",
    "                    AND {job_status}.status = 2\n",
    "                ORDER BY experiment_id, run_id\n",
    "                LIMIT 10;\n",
    "            \"\"\" \n",
    "        ).format(\n",
    "            run=run.identifier,\n",
    "            job_status=job_status.identifier,\n",
    "            job_data=job_data.identifier,\n",
    "            pattern=sql.Literal(\"%energy%\"),\n",
    "        )\n",
    "\n",
    "        with ClientCursor(connection) as c:\n",
    "            print(c.mogrify(query))\n",
    "\n",
    "        with connection.cursor(binary=True) as cursor:\n",
    "            cursor.execute(query, binary=True)\n",
    "            ids = cursor.fetchall()\n",
    "            rows = []\n",
    "            for id in ids:\n",
    "                rows.append(str(id[0]))\n",
    "            return rows\n",
    "        \n",
    "rows = get_ids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34beb455-0bf6-44f7-99df-07b37589b9b50277ac37-8307-4d63-8e67-ecffcce04943\n",
      "\n",
      "d3941040-8f04-4244-a9a2-9e706f244b93\n",
      "81848207-6af1-4a51-a63a-27e05b9ab90e\n",
      "dcc6f804-ee8f-4202-9e00-7c8494f63d3f\n",
      "09de6a5a-48f0-4e63-a759-4c42e6c35d77\n",
      "97320eaf-c8fb-49a9-a537-b6f3870808a2\n",
      "249fb777-e48a-4656-b473-e6966c61d15f\n",
      "943c5132-04c3-44b1-a048-aef110e31a4b\n",
      "dc0e7b9e-c341-40e3-b889-d5fa9a86720f\n",
      "\n",
      "                SELECT\n",
      "                    *\n",
      "                FROM\n",
      "                    run\n",
      "                WHERE run_id = '34beb455-0bf6-44f7-99df-07b37589b9b5';\n",
      "            \n",
      "\n",
      "                SELECT\n",
      "                    *\n",
      "                FROM\n",
      "                    run\n",
      "                WHERE run_id = '97320eaf-c8fb-49a9-a537-b6f3870808a2';\n",
      "            \n",
      "\n",
      "                SELECT\n",
      "                    *\n",
      "                FROM\n",
      "                    run\n",
      "                WHERE run_id = '81848207-6af1-4a51-a63a-27e05b9ab90e';\n",
      "            \n",
      "\n",
      "                SELECT\n",
      "                    *\n",
      "                FROM\n",
      "                    run\n",
      "                WHERE run_id = '09de6a5a-48f0-4e63-a759-4c42e6c35d77';\n",
      "            \n",
      "\n",
      "                SELECT\n",
      "                    *\n",
      "                FROM\n",
      "                    run\n",
      "                WHERE run_id = '0277ac37-8307-4d63-8e67-ecffcce04943';\n",
      "            \n",
      "\n",
      "                SELECT\n",
      "                    *\n",
      "                FROM\n",
      "                    run\n",
      "                WHERE run_id = '943c5132-04c3-44b1-a048-aef110e31a4b';\n",
      "            \n",
      "\n",
      "                SELECT\n",
      "                    *\n",
      "                FROM\n",
      "                    run\n",
      "                WHERE run_id = 'dcc6f804-ee8f-4202-9e00-7c8494f63d3f';\n",
      "            \n",
      "\n",
      "                SELECT\n",
      "                    *\n",
      "                FROM\n",
      "                    run\n",
      "                WHERE run_id = 'd3941040-8f04-4244-a9a2-9e706f244b93';\n",
      "            \n",
      "\n",
      "                SELECT\n",
      "                    *\n",
      "                FROM\n",
      "                    run\n",
      "                WHERE run_id = '249fb777-e48a-4656-b473-e6966c61d15f';\n",
      "            \n",
      "\n",
      "                SELECT\n",
      "                    *\n",
      "                FROM\n",
      "                    run\n",
      "                WHERE run_id = 'dc0e7b9e-c341-40e3-b889-d5fa9a86720f';\n",
      "            \n",
      "                          experiment_id                     run_timestamp  \\\n",
      "0  00007573-edf9-fc11-a260-2144c49569f9  2023-02-09 06:27:32.187240+00:00   \n",
      "\n",
      "                                 run_id                                job_id  \\\n",
      "0  34beb455-0bf6-44f7-99df-07b37589b9b5  34beb455-0bf6-44f7-99df-07b37589b9b5   \n",
      "\n",
      "         seed slurm_job_id task_version num_nodes num_cpus num_gpus  \\\n",
      "0  1666255396     10430277            3         2        2        0   \n",
      "\n",
      "  gpu_memory host_name                   batch  \n",
      "0          0   r2i1n35  optimizer_energy_1_cpu  \n",
      "                          experiment_id                     run_timestamp  \\\n",
      "0  00028376-eb4f-30c8-c93c-c1a61d5fbcd5  2023-02-09 06:01:09.306325+00:00   \n",
      "\n",
      "                                 run_id                                job_id  \\\n",
      "0  97320eaf-c8fb-49a9-a537-b6f3870808a2  97320eaf-c8fb-49a9-a537-b6f3870808a2   \n",
      "\n",
      "         seed slurm_job_id task_version num_nodes num_cpus num_gpus  \\\n",
      "0  1666207730     10430422            3         2        2        0   \n",
      "\n",
      "  gpu_memory host_name                      batch  \n",
      "0          0   r1i4n33  energy_3_cpu_extra_depths  \n",
      "                          experiment_id                     run_timestamp  \\\n",
      "0  00018130-d8da-6c17-1ad8-c7f4bd6188c0  2023-02-09 07:30:44.656374+00:00   \n",
      "\n",
      "                                 run_id                                job_id  \\\n",
      "0  81848207-6af1-4a51-a63a-27e05b9ab90e  81848207-6af1-4a51-a63a-27e05b9ab90e   \n",
      "\n",
      "         seed slurm_job_id task_version num_nodes num_cpus num_gpus  \\\n",
      "0  1666302858     10430364            3         2        2        0   \n",
      "\n",
      "  gpu_memory host_name                   batch  \n",
      "0          0   r2i4n28  optimizer_energy_1_cpu  \n",
      "                          experiment_id                     run_timestamp  \\\n",
      "0  00028376-eb4f-30c8-c93c-c1a61d5fbcd5  2023-02-09 06:01:09.306325+00:00   \n",
      "\n",
      "                                 run_id                                job_id  \\\n",
      "0  09de6a5a-48f0-4e63-a759-4c42e6c35d77  09de6a5a-48f0-4e63-a759-4c42e6c35d77   \n",
      "\n",
      "         seed slurm_job_id task_version num_nodes num_cpus num_gpus  \\\n",
      "0  1666314203     10373666            3         1        2        2   \n",
      "\n",
      "  gpu_memory host_name                      batch  \n",
      "0      13312   r105u25  energy_5_gpu_extra_depths  \n",
      "                          experiment_id                     run_timestamp  \\\n",
      "0  000134bf-ed74-d84f-77b6-67fd4de0f8fa  2023-02-09 05:08:21.176636+00:00   \n",
      "\n",
      "                                 run_id                                job_id  \\\n",
      "0  0277ac37-8307-4d63-8e67-ecffcce04943  0277ac37-8307-4d63-8e67-ecffcce04943   \n",
      "\n",
      "         seed slurm_job_id task_version num_nodes num_cpus num_gpus  \\\n",
      "0  1664223194     10370741            3         2        2        0   \n",
      "\n",
      "  gpu_memory host_name         batch  \n",
      "0          0   r7i2n17  energy_2_cpu  \n",
      "                          experiment_id                     run_timestamp  \\\n",
      "0  00033b8d-feab-63a8-5ce7-538324372f80  2023-02-09 05:37:52.366144+00:00   \n",
      "\n",
      "                                 run_id                                job_id  \\\n",
      "0  943c5132-04c3-44b1-a048-aef110e31a4b  943c5132-04c3-44b1-a048-aef110e31a4b   \n",
      "\n",
      "         seed slurm_job_id task_version num_nodes num_cpus num_gpus  \\\n",
      "0  1666312911     10373526            3         1        2        2   \n",
      "\n",
      "  gpu_memory host_name                      batch  \n",
      "0      13312   r104u03  energy_5_gpu_extra_depths  \n",
      "                          experiment_id                     run_timestamp  \\\n",
      "0  000134bf-ed74-d84f-77b6-67fd4de0f8fa  2023-02-09 05:08:21.176636+00:00   \n",
      "\n",
      "                                 run_id                                job_id  \\\n",
      "0  d3941040-8f04-4244-a9a2-9e706f244b93  d3941040-8f04-4244-a9a2-9e706f244b93   \n",
      "\n",
      "         seed slurm_job_id task_version num_nodes num_cpus num_gpus  \\\n",
      "0  1663283653     10373627            3         1        2        2   \n",
      "\n",
      "  gpu_memory host_name     batch  \n",
      "0      13312   r105u01  energy_1  \n",
      "                          experiment_id                     run_timestamp  \\\n",
      "0  00033b8d-feab-63a8-5ce7-538324372f80  2023-02-09 05:37:52.366144+00:00   \n",
      "\n",
      "                                 run_id                                job_id  \\\n",
      "0  dc0e7b9e-c341-40e3-b889-d5fa9a86720f  dc0e7b9e-c341-40e3-b889-d5fa9a86720f   \n",
      "\n",
      "         seed slurm_job_id task_version num_nodes num_cpus num_gpus  \\\n",
      "0  1666206438     10430605            3         2        2        0   \n",
      "\n",
      "  gpu_memory host_name                      batch  \n",
      "0          0    r6i7n2  energy_3_cpu_extra_depths  \n",
      "                          experiment_id                     run_timestamp  \\\n",
      "0  0001ebca-a503-2f38-30a8-61b07edcf00a  2023-02-09 07:31:02.690649+00:00   \n",
      "\n",
      "                                 run_id                                job_id  \\\n",
      "0  dcc6f804-ee8f-4202-9e00-7c8494f63d3f  dcc6f804-ee8f-4202-9e00-7c8494f63d3f   \n",
      "\n",
      "         seed slurm_job_id task_version num_nodes num_cpus num_gpus  \\\n",
      "0  1666348759     10431060            3         2        2        0   \n",
      "\n",
      "  gpu_memory host_name                   batch  \n",
      "0          0   r1i4n20  optimizer_energy_1_cpu  \n",
      "                          experiment_id                     run_timestamp  \\\n",
      "0  0002fb63-186e-1d5c-1c4e-673ab0223e4c  2023-02-09 06:32:47.030989+00:00   \n",
      "\n",
      "                                 run_id                                job_id  \\\n",
      "0  249fb777-e48a-4656-b473-e6966c61d15f  249fb777-e48a-4656-b473-e6966c61d15f   \n",
      "\n",
      "         seed slurm_job_id task_version num_nodes num_cpus num_gpus  \\\n",
      "0  1666300767     10430443            3         2        2        0   \n",
      "\n",
      "  gpu_memory host_name                   batch  \n",
      "0          0   r4i0n29  optimizer_energy_1_cpu  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None, None, None, None, None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "def get_data(run_id):\n",
    "    print(run_id)\n",
    "    with ConnectionManager(credentials) as connection:\n",
    "        \"\"\"\n",
    "        TODO: convert this into two steps:\n",
    "        1) Get the ids of the runs we want to extract (in energy batches) ordered by experiment_id\n",
    "\n",
    "            SELECT\n",
    "                    {run}.{run_id}\n",
    "            FROM\n",
    "                    {run},\n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "            WHERE TRUE\n",
    "                    AND {run}.batch like {pattern}\n",
    "                    AND {job_status}.id = {run}.run_id\n",
    "                    AND {job_status}.id = {job_data}.id\n",
    "                AND {job_status}.status = 2\n",
    "            ORDER BY experiment_id, run_id;\n",
    "\n",
    "        2) In parallel (using the multiprocessing lib), extract blocks of ids into a partitioned parquet file\n",
    "        -> partition by the attributes we care about querying by (dataset, size, shape, depth)\n",
    "\n",
    "                SELECT\n",
    "                    {columns}\n",
    "                FROM\n",
    "                    {run},\n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "                WHERE TRUE\n",
    "                    AND {job_status}.id = {run}.run_id\n",
    "                    AND {job_status}.id = {job_data}.id\n",
    "                    AND {job_status}.status = 2\n",
    "                    AND {run}.{run_id} IN ({ids})\n",
    "\n",
    "            pool = multiprocessing.ProcessPool(multiprocessing.cpu_count())\n",
    "            results = pool.uimap(download_chunk, chunks)\n",
    "            for num_rows, chunk in results:\n",
    "                num_stored += 1\n",
    "                print(f\"Stored {num_rows} in chunk {chunk}, {num_stored} / {len(chunks)}.\")\n",
    "\n",
    "        extra credit) extract butter data matching this as well into a new dataset\n",
    "        extra credit) make a summary dataset that summarizes the quartiles of # epochs to reach target test loss levels\n",
    "        \"\"\"\n",
    "        query = psycopg.sql.SQL(\n",
    "            \"\"\"\n",
    "                SELECT\n",
    "                    *\n",
    "                FROM\n",
    "                    run\n",
    "                WHERE run_id = {run_id};\n",
    "            \"\"\" \n",
    "        ).format(\n",
    "            columns=columns.columns_sql,\n",
    "            run_id=sql.Literal(run_id),\n",
    "        )\n",
    "\n",
    "        with ClientCursor(connection) as c:\n",
    "            print(c.mogrify(query))\n",
    "\n",
    "        with connection.cursor(binary=True) as cursor:\n",
    "            cursor.execute(query, binary=True)\n",
    "            rows = cursor.fetchall()\n",
    "            rows = [[str(item) for item in row] for row in rows]\n",
    "\n",
    "            column_names = [\"experiment_id\", \"run_timestamp\", \"run_id\", \"job_id\", \"seed\", \"slurm_job_id\", \"task_version\", \"num_nodes\", \"num_cpus\", \"num_gpus\", \"gpu_memory\", \"host_name\", \"batch\", \"run_data\", \"run_history\", \"run_extended_history\", \"task\"]\n",
    "            df = pd.DataFrame(rows, columns=column_names)\n",
    "\n",
    "            # convert all columns to strings and drop \"run_data\", \"run_history\", \"run_extended_history\", \"task\"\n",
    "            df = df.drop(columns=[ \"run_data\", \"run_history\", \"run_extended_history\", \"task\", \"run_data\", \"run_history\", \"run_extended_history\", \"task\"])\n",
    "            print(df)\n",
    "\n",
    "            table = pa.Table.from_pandas(df)\n",
    "            pq.write_to_dataset(table, root_path='.', partition_cols=['batch', 'num_nodes', 'num_gpus', 'num_cpus'])\n",
    "\n",
    "\n",
    "# run this in parallel\n",
    "import multiprocessing.pool as pool\n",
    "pool = pool.ThreadPool(10)\n",
    "pool.map(get_data, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
