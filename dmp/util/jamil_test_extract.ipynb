{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-17 14:17:06.968352: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "import io\n",
    "import uuid\n",
    "import psycopg.sql\n",
    "import pyarrow\n",
    "import pyarrow.parquet\n",
    "\n",
    "import jobqueue\n",
    "from jobqueue.connection_manager import ConnectionManager\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "\n",
    "from typing import Callable, List\n",
    "\n",
    "from psycopg import sql\n",
    "\n",
    "import dmp.keras_interface.model_serialization as model_serialization\n",
    "from dmp.task.experiment.training_experiment.training_epoch import TrainingEpoch\n",
    "from dmp.postgres_interface.element.column import Column\n",
    "from dmp.postgres_interface.element.table import Table\n",
    "from dmp.postgres_interface.element.column_group import ColumnGroup\n",
    "\n",
    "from dmp.util.butter_e_export import *\n",
    "\n",
    "pd.options.display.max_seq_items = None\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "credentials = jobqueue.load_credentials(\"dmp\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: convert this into two steps:\n",
    "    1) Get the ids of the runs we want to extract (in energy batches) ordered by experiment_id\n",
    "\n",
    "          SELECT\n",
    "                  {run}.{run_id}\n",
    "          FROM\n",
    "                  {run},\n",
    "                  {job_status},\n",
    "                  {job_data}\n",
    "          WHERE TRUE\n",
    "                  AND {run}.batch like {pattern}\n",
    "                  AND {job_status}.id = {run}.run_id\n",
    "                  AND {job_status}.id = {job_data}.id\n",
    "              AND {job_status}.status = 2\n",
    "          ORDER BY experiment_id, run_id;\n",
    "\n",
    "    2) In parallel (using the multiprocessing lib), extract blocks of ids into a partitioned parquet file\n",
    "      -> partition by the attributes we care about querying by (dataset, size, shape, depth)\n",
    "\n",
    "              SELECT\n",
    "                  {columns}\n",
    "              FROM\n",
    "                  {run},\n",
    "                  {job_status},\n",
    "                  {job_data}\n",
    "              WHERE TRUE\n",
    "                  AND {job_status}.id = {run}.run_id\n",
    "                  AND {job_status}.id = {job_data}.id\n",
    "                  AND {job_status}.status = 2\n",
    "                  AND {run}.{run_id} IN ({ids})\n",
    "\n",
    "        pool = multiprocessing.ProcessPool(multiprocessing.cpu_count())\n",
    "        results = pool.uimap(download_chunk, chunks)\n",
    "        for num_rows, chunk in results:\n",
    "            num_stored += 1\n",
    "            print(f\"Stored {num_rows} in chunk {chunk}, {num_stored} / {len(chunks)}.\")\n",
    "\n",
    "    extra credit) extract butter data matching this as well into a new dataset\n",
    "    extra credit) make a summary dataset that summarizes the quartiles of # epochs to reach target test loss levels\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run vars {'_name': 'run', '_columns': (), '_index': None}\n",
      "('experiment_id', 'run_timestamp', 'run_id', 'job_id', 'seed', 'slurm_job_id', 'task_version', 'num_nodes', 'num_cpus', 'num_gpus', 'gpu_memory', 'host_name', 'batch', 'run_data', 'run_history', 'run_extended_history', 'queue', 'status', 'priority', 'start_time', 'update_time', 'worker', 'error_count', 'error', 'parent', 'command')\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n",
      "{Column(_name='experiment_id', type_name='uuid'): 0, Column(_name='run_timestamp', type_name='timestamp'): 1, Column(_name='run_id', type_name='uuid'): 2, Column(_name='job_id', type_name='uuid'): 3, Column(_name='seed', type_name='bigint'): 4, Column(_name='slurm_job_id', type_name='bigint'): 5, Column(_name='task_version', type_name='smallint'): 6, Column(_name='num_nodes', type_name='smallint'): 7, Column(_name='num_cpus', type_name='smallint'): 8, Column(_name='num_gpus', type_name='smallint'): 9, Column(_name='gpu_memory', type_name='integer'): 10, Column(_name='host_name', type_name='text'): 11, Column(_name='batch', type_name='text'): 12, Column(_name='run_data', type_name='jsonb'): 13, Column(_name='run_history', type_name='bytea'): 14, Column(_name='run_extended_history', type_name='bytea'): 15, Column(_name='queue', type_name='smallint'): 16, Column(_name='status', type_name='smallint'): 17, Column(_name='priority', type_name='integer'): 18, Column(_name='start_time', type_name='timestamp'): 19, Column(_name='update_time', type_name='timestamp'): 20, Column(_name='worker', type_name='uuid'): 21, Column(_name='error_count', type_name='smallint'): 22, Column(_name='error', type_name='text'): 23, Column(_name='parent', type_name='uuid'): 24, Column(_name='command', type_name='jsonb'): 25}\n"
     ]
    }
   ],
   "source": [
    "from psycopg import ClientCursor\n",
    "\n",
    "\n",
    "print(f\"run vars {vars(run)}\")\n",
    "\n",
    "columns = (\n",
    "    run\n",
    "    + ColumnGroup(*[c for c in job_status.columns if c.name != \"id\"])\n",
    "    + job_data.command\n",
    ")\n",
    "print(columns.names)\n",
    "\n",
    "\n",
    "def passthrough(row, index, value, column, data):\n",
    "    data[column.name] = value\n",
    "\n",
    "\n",
    "column_converters: List[Callable] = [passthrough for _ in columns]\n",
    "\n",
    "\n",
    "def flatten_json(json_obj, destination=None, parent_key=\"\", separator=\"_\"):\n",
    "    if isinstance(destination, dict):\n",
    "        flattened = destination\n",
    "    else:\n",
    "        flattened = {}\n",
    "\n",
    "    for key, value in json_obj.items():\n",
    "        new_key = f\"{parent_key}{separator}{key}\" if parent_key else key\n",
    "        if isinstance(value, dict):\n",
    "            flattened.update(flatten_json(value, new_key, separator=separator))\n",
    "        else:\n",
    "            flattened[new_key] = value\n",
    "    return flattened\n",
    "\n",
    "\n",
    "column_converters[\n",
    "    columns.get_index_of(job_data.command)\n",
    "] = lambda row, index, value, column, data: flatten_json(value, destination=data)\n",
    "column_converters[\n",
    "    columns.get_index_of(run.run_data)\n",
    "] = lambda row, index, value, column, data: flatten_json(value, destination=data)\n",
    "\n",
    "\n",
    "def parquet_to_dataframe(row, index, value, column, data):\n",
    "    with io.BytesIO(value) as buffer:\n",
    "        data[column.name] = (\n",
    "            pyarrow.parquet.read_table(pyarrow.PythonFile(buffer, mode=\"r\"))\n",
    "            .to_pandas()\n",
    "            .sort_values(by=\"epoch\")\n",
    "        )\n",
    "\n",
    "\n",
    "column_converters[columns.get_index_of(run.run_history)] = parquet_to_dataframe\n",
    "column_converters[columns.get_index_of(run.run_extended_history)] = parquet_to_dataframe\n",
    "\n",
    "\n",
    "dfs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                SELECT\n",
      "                    \"run\".run_id\n",
      "                FROM\n",
      "                    \"run\",\n",
      "                    \"job_status\",\n",
      "                    \"job_data\"\n",
      "                WHERE TRUE\n",
      "                    AND \"run\".batch like '%energy%'\n",
      "                    AND \"job_status\".id = \"run\".run_id\n",
      "                    AND \"job_status\".id = \"job_data\".id\n",
      "                    AND \"job_status\".status = 2\n",
      "                    AND \"job_data\".command @> '{\n",
      "            \"model\": {\n",
      "                \"shape\": \"rectangle\",\n",
      "                \"depth\": 5\n",
      "            },\n",
      "            \"dataset\": {\n",
      "                \"name\":\"sleep\"\n",
      "            }\n",
      "        }'::jsonb\n",
      "                ORDER BY experiment_id, run_id\n",
      "                LIMIT 10;\n",
      "            \n",
      "['50c8b611-a33e-40c8-9756-5a86da78235b', '8472b423-9e1a-4d1a-9d87-14d6154c3016', '6da9a6ae-ea1d-418e-bdac-4a196fe3765b', 'a56ccce5-5090-469d-b47f-cce5dc8003ec', '787b0956-a913-42cf-b1c9-814a2e383233', 'ca64ea3e-4b8a-45b8-b8c0-c5ba97f919a7', '02cf0870-c811-46a0-a9f0-cc43c11d4581', '100650b3-1b07-4ff3-aff1-7d6066cce106', '663b243e-cb2b-4b3b-842c-a225095d977f', '7ebf1c91-15db-41df-a502-a05272e9103c']\n"
     ]
    }
   ],
   "source": [
    "def get_ids():\n",
    "    with ConnectionManager(credentials) as connection:\n",
    "        \"\"\"\n",
    "        TODO: convert this into two steps:\n",
    "        1) Get the ids of the runs we want to extract (in energy batches) ordered by experiment_id\n",
    "\n",
    "            SELECT\n",
    "                    {run}.{run_id}\n",
    "            FROM\n",
    "                    {run},\n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "            WHERE TRUE\n",
    "                    AND {run}.batch like {pattern}\n",
    "                    AND {job_status}.id = {run}.run_id\n",
    "                    AND {job_status}.id = {job_data}.id\n",
    "                AND {job_status}.status = 2\n",
    "            ORDER BY experiment_id, run_id;\n",
    "\n",
    "        2) In parallel (using the multiprocessing lib), extract blocks of ids into a partitioned parquet file\n",
    "        -> partition by the attributes we care about querying by (dataset, size, shape, depth)\n",
    "\n",
    "                SELECT\n",
    "                    {columns}\n",
    "                FROM\n",
    "                    {run},\n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "                WHERE TRUE\n",
    "                    AND {job_status}.id = {run}.run_id\n",
    "                    AND {job_status}.id = {job_data}.id\n",
    "                    AND {job_status}.status = 2\n",
    "                    AND {run}.{run_id} IN ({ids})\n",
    "\n",
    "            pool = multiprocessing.ProcessPool(multiprocessing.cpu_count())\n",
    "            results = pool.uimap(download_chunk, chunks)\n",
    "            for num_rows, chunk in results:\n",
    "                num_stored += 1\n",
    "                print(f\"Stored {num_rows} in chunk {chunk}, {num_stored} / {len(chunks)}.\")\n",
    "\n",
    "        extra credit) extract butter data matching this as well into a new dataset\n",
    "        extra credit) make a summary dataset that summarizes the quartiles of # epochs to reach target test loss levels\n",
    "        \"\"\"\n",
    "        query = psycopg.sql.SQL(\n",
    "            \"\"\"\n",
    "                SELECT\n",
    "                    {run}.run_id\n",
    "                FROM\n",
    "                    {run},\n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "                WHERE TRUE\n",
    "                    AND {run}.batch like {pattern}\n",
    "                    AND {job_status}.id = {run}.run_id\n",
    "                    AND {job_status}.id = {job_data}.id\n",
    "                    AND {job_status}.status = 2\n",
    "                    AND {job_data}.command @> {json_data}::jsonb\n",
    "                ORDER BY experiment_id, run_id\n",
    "                LIMIT 10;\n",
    "            \"\"\" \n",
    "        ).format(\n",
    "            run=run.identifier,\n",
    "            job_status=job_status.identifier,\n",
    "            job_data=job_data.identifier,\n",
    "            pattern=sql.Literal(\"%energy%\"),\n",
    "            json_data=sql.Literal('''{\n",
    "            \"model\": {\n",
    "                \"shape\": \"rectangle\",\n",
    "                \"depth\": 5\n",
    "            },\n",
    "            \"dataset\": {\n",
    "                \"name\":\"sleep\"\n",
    "            }\n",
    "        }'''\n",
    "\n",
    "            )\n",
    "        )\n",
    "\n",
    "        with ClientCursor(connection) as c:\n",
    "            print(c.mogrify(query))\n",
    "\n",
    "        with connection.cursor(binary=True) as cursor:\n",
    "            cursor.execute(query, binary=True)\n",
    "            ids = cursor.fetchall()\n",
    "            rows = []\n",
    "            for id in ids:\n",
    "                rows.append(str(id[0]))\n",
    "            return rows\n",
    "        \n",
    "rows = get_ids()\n",
    "print(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['caa96421-58f6-435e-afa3-7bcf26b8af56',\n",
       " '9329da14-34c9-45ec-8e85-a59c4403767a',\n",
       " 'ce53fcdf-2220-47b7-9b88-67b137a3775a',\n",
       " '0b1ae00c-8891-429f-bd78-3ec403102d95',\n",
       " 'df1e243e-7296-4c05-b2b1-b9240f90c34c',\n",
       " '0ff5a77f-5154-42ba-891f-5764f95a5879',\n",
       " '44a97541-44fe-477f-bf3c-43b8cccd33fa',\n",
       " '07700714-85d3-4a1a-a0c3-3070c91cb0e4',\n",
       " '7bb1f18f-da22-4370-8cfe-751c83113822',\n",
       " 'c411140c-ae39-413c-88da-742cac2714cb']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50c8b611-a33e-40c8-9756-5a86da78235b\n",
      "\n",
      "                SELECT\n",
      "                    \"experiment_id\",\"run_timestamp\",\"run_id\",\"job_id\",\"seed\",\"slurm_job_id\",\"task_version\",\"num_nodes\",\"num_cpus\",\"num_gpus\",\"gpu_memory\",\"host_name\",\"batch\",\"run_data\",\"run_history\",\"run_extended_history\",\"queue\",\"status\",\"priority\",\"start_time\",\"update_time\",\"worker\",\"error_count\",\"error\",\"parent\",\"command\"\n",
      "                FROM\n",
      "                    \"run\", \n",
      "                    \"job_status\",\n",
      "                    \"job_data\"\n",
      "                WHERE run_id = '50c8b611-a33e-40c8-9756-5a86da78235b';\n",
      "            \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/jgafur/Desktop/Jamil_Documentation/Modular ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m             rows \u001b[39m=\u001b[39m cursor\u001b[39m.\u001b[39mfetchall()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m             \u001b[39mprint\u001b[39m(rows)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=73'>74</a>\u001b[0m get_data(rows[\u001b[39m0\u001b[39;49m])\n",
      "\u001b[1;32m/Users/jgafur/Desktop/Jamil_Documentation/Modular ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb Cell 6\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_data\u001b[39m(run_id):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(run_id)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39mwith\u001b[39;49;00m ConnectionManager(credentials) \u001b[39mas\u001b[39;49;00m connection:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m        \u001b[39;49m\u001b[39m\"\"\"\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m        TODO: convert this into two steps:\u001b[39;49;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m        1) Get the ids of the runs we want to extract (in energy batches) ordered by experiment_id\u001b[39;49;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39m        extra credit) make a summary dataset that summarizes the quartiles of # epochs to reach target test loss levels\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39m        \"\"\"\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m         query \u001b[39m=\u001b[39;49m psycopg\u001b[39m.\u001b[39;49msql\u001b[39m.\u001b[39;49mSQL(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39m            \u001b[39;49m\u001b[39m\"\"\"\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39m                SELECT\u001b[39;49;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m             job_status\u001b[39m=\u001b[39;49mjob_status\u001b[39m.\u001b[39;49midentifier,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m         )\n",
      "File \u001b[0;32m~/Desktop/Jamil_Documentation/Modular ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/src/jobqueue/jobqueue/connection_manager.py:67\u001b[0m, in \u001b[0;36mConnectionManager.__exit__\u001b[0;34m(self, exception_type, exception_value, traceback)\u001b[0m\n\u001b[1;32m     64\u001b[0m         do_and_capture(\u001b[39mlambda\u001b[39;00m: connection\u001b[39m.\u001b[39mclose())\n\u001b[1;32m     66\u001b[0m \u001b[39mif\u001b[39;00m exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mraise\u001b[39;00m exception\n\u001b[1;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;32m/Users/jgafur/Desktop/Jamil_Documentation/Modular ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb Cell 6\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m     \u001b[39mprint\u001b[39m(c\u001b[39m.\u001b[39mmogrify(query))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mwith\u001b[39;00m connection\u001b[39m.\u001b[39mcursor(binary\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m cursor:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m     cursor\u001b[39m.\u001b[39;49mexecute(query, binary\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     rows \u001b[39m=\u001b[39m cursor\u001b[39m.\u001b[39mfetchall()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/jgafur/Desktop/Jamil_Documentation/Modular%20ML/BUTTER-Empirical-Deep-Learning-Experimental-Framework/dmp/util/jamil_test_extract.ipynb#W4sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39mprint\u001b[39m(rows)\n",
      "File \u001b[0;32m~/anaconda3/envs/dmp/lib/python3.11/site-packages/psycopg/cursor.py:737\u001b[0m, in \u001b[0;36mCursor.execute\u001b[0;34m(self, query, params, prepare, binary)\u001b[0m\n\u001b[1;32m    733\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conn\u001b[39m.\u001b[39mwait(\n\u001b[1;32m    734\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_execute_gen(query, params, prepare\u001b[39m=\u001b[39mprepare, binary\u001b[39m=\u001b[39mbinary)\n\u001b[1;32m    735\u001b[0m         )\n\u001b[1;32m    736\u001b[0m \u001b[39mexcept\u001b[39;00m e\u001b[39m.\u001b[39m_NO_TRACEBACK \u001b[39mas\u001b[39;00m ex:\n\u001b[0;32m--> 737\u001b[0m     \u001b[39mraise\u001b[39;00m ex\u001b[39m.\u001b[39mwith_traceback(\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    738\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "def get_data(run_id):\n",
    "    print(run_id)\n",
    "    with ConnectionManager(credentials) as connection:\n",
    "        \"\"\"\n",
    "        TODO: convert this into two steps:\n",
    "        1) Get the ids of the runs we want to extract (in energy batches) ordered by experiment_id\n",
    "\n",
    "            SELECT\n",
    "                    {run}.{run_id}\n",
    "            FROM\n",
    "                    {run},\n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "            WHERE TRUE\n",
    "                    AND {run}.batch like {pattern}\n",
    "                    AND {job_status}.id = {run}.run_id\n",
    "                    AND {job_status}.id = {job_data}.id\n",
    "                AND {job_status}.status = 2\n",
    "            ORDER BY experiment_id, run_id;\n",
    "\n",
    "        2) In parallel (using the multiprocessing lib), extract blocks of ids into a partitioned parquet file\n",
    "        -> partition by the attributes we care about querying by (dataset, size, shape, depth)\n",
    "\n",
    "                SELECT\n",
    "                    {columns}\n",
    "                FROM\n",
    "                    {run},\n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "                WHERE TRUE\n",
    "                    AND {job_status}.id = {run}.run_id\n",
    "                    AND {job_status}.id = {job_data}.id\n",
    "                    AND {job_status}.status = 2\n",
    "                    AND {run}.{run_id} IN ({ids})\n",
    "\n",
    "            pool = multiprocessing.ProcessPool(multiprocessing.cpu_count())\n",
    "            results = pool.uimap(download_chunk, chunks)\n",
    "            for num_rows, chunk in results:\n",
    "                num_stored += 1\n",
    "                print(f\"Stored {num_rows} in chunk {chunk}, {num_stored} / {len(chunks)}.\")\n",
    "\n",
    "        extra credit) extract butter data matching this as well into a new dataset\n",
    "        extra credit) make a summary dataset that summarizes the quartiles of # epochs to reach target test loss levels\n",
    "        \"\"\"\n",
    "        query = psycopg.sql.SQL(\n",
    "            \"\"\"\n",
    "                SELECT\n",
    "                    {columns}\n",
    "                FROM\n",
    "                    {run}, \n",
    "                    {job_status},\n",
    "                    {job_data}\n",
    "                WHERE run_id = {run_id};\n",
    "            \"\"\" \n",
    "        ).format(\n",
    "            columns=columns.columns_sql,\n",
    "            run_id=sql.Literal(run_id),\n",
    "            run=run.identifier,\n",
    "            job_data=job_data.identifier,\n",
    "            job_status=job_status.identifier,\n",
    "        )\n",
    "\n",
    "        with ClientCursor(connection) as c:\n",
    "            print(c.mogrify(query))\n",
    "\n",
    "        with connection.cursor(binary=True) as cursor:\n",
    "            cursor.execute(query, binary=True)\n",
    "            rows = cursor.fetchall()\n",
    "            print(rows)\n",
    "\n",
    "get_data(rows[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./batch=energy_1/num_nodes=1/num_gpus=2/num_cpus=2/56e4c12eae784e308d8422920a916930-0.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyarrow read parquet from path as table\n",
    "table = pq.read_table(path)\n",
    "pd = table.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment_id</th>\n",
       "      <th>run_timestamp</th>\n",
       "      <th>run_id</th>\n",
       "      <th>job_id</th>\n",
       "      <th>seed</th>\n",
       "      <th>slurm_job_id</th>\n",
       "      <th>task_version</th>\n",
       "      <th>gpu_memory</th>\n",
       "      <th>host_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>44349db1-47c2-1f11-8011-14328aef8e23</td>\n",
       "      <td>2023-02-09 05:56:23.541736+00:00</td>\n",
       "      <td>a56ccce5-5090-469d-b47f-cce5dc8003ec</td>\n",
       "      <td>a56ccce5-5090-469d-b47f-cce5dc8003ec</td>\n",
       "      <td>1663278620</td>\n",
       "      <td>10373625</td>\n",
       "      <td>3</td>\n",
       "      <td>13312</td>\n",
       "      <td>r104u37</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          experiment_id                     run_timestamp  \\\n",
       "0  44349db1-47c2-1f11-8011-14328aef8e23  2023-02-09 05:56:23.541736+00:00   \n",
       "\n",
       "                                 run_id                                job_id  \\\n",
       "0  a56ccce5-5090-469d-b47f-cce5dc8003ec  a56ccce5-5090-469d-b47f-cce5dc8003ec   \n",
       "\n",
       "         seed slurm_job_id task_version gpu_memory host_name  \n",
       "0  1663278620     10373625            3      13312   r104u37  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
