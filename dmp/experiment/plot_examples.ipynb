{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations in Plotly for the DMP Aspect Test Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "from dmp.data.logging import _get_sql_engine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from joblib import Memory\n",
    "from ipywidgets import interact, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "import scipy.interpolate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = _get_sql_engine()\n",
    "\n",
    "memory = Memory(location=\"./cache\", verbose=0)\n",
    "\n",
    "def query(query_string):\n",
    "    with db.connect() as engine:\n",
    "        return pd.read_sql(query_string, engine)\n",
    "\n",
    "cached_query = memory.cache(query)\n",
    "\n",
    "def clear_cache():\n",
    "    memory.clear()\n",
    "    \n",
    "options = {\n",
    "    \"dataset\": ['529_pollen',\n",
    "        'sleep',\n",
    "#         'adult',\n",
    "        '537_houses',\n",
    "#         'nursery',\n",
    "        '201_pol',\n",
    "        'mnist',\n",
    "        'connect_4',\n",
    "        'wine_quality_white'],\n",
    "    \"agg\": [\"avg\", \"min\", \"max\"],\n",
    "    \"topology\" : [\n",
    "        \"rectangle\", \"trapezoid\", \"exponential\",\n",
    "        \"wide_first_2x\", \"wide_first_4x\", \"wide_first\"],\n",
    "     \"loss\": ['loss', 'hinge', 'accuracy',\n",
    "       'val_loss', 'val_hinge', 'val_accuracy',\n",
    "       'squared_hinge', 'cosine_similarity',\n",
    "       'val_squared_hinge', 'mean_squared_error',\n",
    "       'mean_absolute_error', 'val_cosine_similarity',\n",
    "       'val_mean_squared_error', 'root_mean_squared_error',\n",
    "       'val_mean_absolute_error',\n",
    "       'kullback_leibler_divergence',\n",
    "       'val_root_mean_squared_error',\n",
    "       'mean_squared_logarithmic_error',\n",
    "       'val_kullback_leibler_divergence',\n",
    "       'val_mean_squared_logarithmic_error'],\n",
    "    \"residual_mode\": [\"none\", \"full\"],\n",
    "    \"group_select\" : [\"min\", \"max\"],\n",
    "    'depth':[2, 3, 4, 5, 6, 7, 8, 9, 10, 12, 14, 16, 18, 20],\n",
    "    'budget': [32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384,\n",
    "                32768, 65536, 131072, 262144, 524288, 1048576, 2097152, 4194304,\n",
    "                8388608, 16777216, 33554432],\n",
    "    'color_range': (1.05, 2.5, .05),\n",
    "    'epoch_axis' : ['epoch', 'log_effort', 'effort'],\n",
    "    'learning_rate': [0.001, 0.0001],\n",
    "    'label_noise': [0.0, 0.05, 0.10, 0.15, 0.20],\n",
    "    'groupname' : ['fixed_3k_1', 'fixed_3k_0', 'fixed_01', 'exp00', 'exp01']\n",
    "    \n",
    "}\n",
    "\n",
    "import plotly.io as pio\n",
    "pio.templates[\"dmp_template\"] = go.layout.Template(\n",
    "    layout=go.Layout(\n",
    "        colorscale={\n",
    "            'diverging':'Temps',\n",
    "            'sequential': 'Viridis_r',\n",
    "            'sequentialminus':'Viridis_r',\n",
    "#             'sequential': 'Agsunset_r',\n",
    "#             'sequentialminus':'Agsunset_r',\n",
    "        }\n",
    "    )\n",
    ")\n",
    "pio.templates.default = 'dmp_template'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:[Memory(location=./cache/joblib)]: Flushing completely the cache\n"
     ]
    }
   ],
   "source": [
    "clear_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'exp00': 1309, '529_pollen': 1118, 'wine_quality_white': 1119, 'wide_first': 1120, 'exponential': 1123, 'trapezoid': 1124, 'full': 1125, 'adult': 1316, '537_houses': 1122, 'connect_4': 1126, 'mnist': 1127, 'nursery': 1327, 'sleep': 1131, 'exp01': 1328, 'fixed_3k_1': 1176, 'fixed_01': 1340, 'wide_first_4x': 1213, 'wide_first_2x': 1214, 'fixed_3k_0': 1112, '201_pol': 1113, 'rectangle': 1114, 'none': 1115, 'adam': 1116, 'relu': 1117}\n"
     ]
    }
   ],
   "source": [
    "string_map_df = pd.read_sql(\n",
    "        f'''SELECT id, value from strings''',\n",
    "        db.execution_options(stream_results=True, postgresql_with_hold=True), coerce_float=False,\n",
    "        params=())\n",
    "\n",
    "string_to_id_map = {}\n",
    "id_to_string_map = {}\n",
    "\n",
    "values = string_map_df['value'].to_list()\n",
    "for i, str_id in enumerate(string_map_df['id'].to_list()):\n",
    "    string_to_id_map[values[i]] = str_id\n",
    "    id_to_string_map[str_id] = values[i]\n",
    "    \n",
    "def string_to_id(s):\n",
    "    if isinstance(s, str):\n",
    "        return string_to_id_map[s]\n",
    "    return [string_to_id(e) for e in s]\n",
    "\n",
    "def id_to_string(i):\n",
    "    if isinstance(i, int):\n",
    "        return id_to_string_map[i]\n",
    "    return [id_to_string(e) for e in i]\n",
    "\n",
    "print(string_to_id_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_value(df, loss, color_range):\n",
    "    z_label = loss\n",
    "    #     df[\"value\"] = -np.log(np.minimum(df[\"value\"], np.min(df[\"value\"])*4))   \n",
    "#     df[\"value\"] = -np.log(df['value'] / np.min(df['value']))\n",
    "#     df[\"value\"] = -np.log(df['value'] / np.min(df['value']))\n",
    "#     df[\"value\"] = -(df['value'] / np.min(df['value']))\n",
    "\n",
    "    minimizing = True\n",
    "    if 'accuracy' in loss:\n",
    "        df['value'] = 1 - df['value']\n",
    "        df['value'] = np.minimum(np.min(df['value']) * color_range, df['value'])\n",
    "        df[\"value\"] = np.log(df['value'])/np.log(10)\n",
    "        z_label = f'log(1-{loss})'\n",
    "    elif 'loss' in loss:\n",
    "#         df[\"value\"] = -np.exp(1 - df['value']/np.min(df['value']))\n",
    "        df['value'] = np.minimum(np.min(df['value']) * color_range, df['value'])\n",
    "#         df[\"value\"] = -np.log(df['value'])/np.log(10)\n",
    "#         z_label = f'-log({loss})'\n",
    "        \n",
    "        df[\"value\"] = df['value'] / np.abs(np.min(df['value']))\n",
    "        z_label = f'loss / abs(min(loss))'\n",
    "#         df[\"value\"] = -np.log(df['value'] / np.min(df['value']))/np.log(10)\n",
    "#         df[\"value\"] = -df['value'] / np.min(df['value'])\n",
    "    elif 'error' in loss:\n",
    "        df[\"value\"] = df['value'] / np.min(df['value'])\n",
    "        df['value'] = np.minimum(color_range, df['value'])\n",
    "        z_label = f'error / min(error)'\n",
    "    \n",
    "    if minimizing:\n",
    "        best = np.nanmin(df['value'])\n",
    "    else:\n",
    "        best = np.nanmax(df['value'])\n",
    "        \n",
    "    return z_label, minimizing, best\n",
    "\n",
    "def compute_effort(df):\n",
    "    df[\"effort\"] = (df[\"epoch\"] * df[\"budget\"].astype(\"float\")).astype(\"float\")\n",
    "    df[\"log_effort\"] = np.log(df[\"effort\"]) / np.log(10)\n",
    "    df['relative_effort'] = df['effort'] / np.min(df['effort'])\n",
    "    df['relative_log_effort'] = np.log(df['relative_effort']) / np.log(10)\n",
    "\n",
    "\n",
    "def get_values_for_categorical_keys(df, partition_keys):\n",
    "    partitions = []\n",
    "    for partition_key in partition_keys:\n",
    "        partition_values = sorted(df[partition_key].unique())\n",
    "        partitions.append(\n",
    "            (partition_key,\n",
    "            {key : index for index, key in enumerate(partition_values)},\n",
    "            partition_values\n",
    "            ))\n",
    "    return tuple(partitions)\n",
    "\n",
    "def partitioned_interpolation(df, partition_keys, interpolation_key, value_key, resolution):\n",
    "    partitions = get_values_for_categorical_keys(df, partition_keys)\n",
    "    \n",
    "    def make_partition_accumulator(i):\n",
    "        index = partitions[i][1]\n",
    "        return [make_partition_accumulator(i + 1) if i < len(partitions) - 1 else ([], []) \n",
    "                for p in range(len(index))]\n",
    "    acc = make_partition_accumulator(0)\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        a = acc\n",
    "        for partition_key, index, _ in partitions:\n",
    "            a = a[index[row[partition_key]]]\n",
    "        a[0].append(row[interpolation_key])\n",
    "        a[1].append(row[value_key])\n",
    "\n",
    "    interpolation_series = df[interpolation_key]\n",
    "    interpolation_index = np.linspace(np.min(interpolation_series), np.max(interpolation_series), resolution)\n",
    "    partition_indexes = [np.linspace(0, len(p), len(p)) for p in partitions]\n",
    "    \n",
    "    def do_interpolation(a):\n",
    "        if type(a) is list:\n",
    "            return [do_interpolation(p) for p in a]\n",
    "        func = scipy.interpolate.interp1d(a[0], a[1], kind='linear', bounds_error=False, fill_value=np.NaN)\n",
    "        return func(interpolation_index)            \n",
    "    interpolated = np.array(do_interpolation(acc))\n",
    "    print(f'interpolated {interpolated.shape}')\n",
    "    return partitions, interpolation_index, interpolated\n",
    "\n",
    "\n",
    "def make_2d_heatmap_viz(df, group, dataset, topology, loss, agg, residual_mode, viz, color_range):\n",
    "    z_label, minimizing, best = setup_value(df, loss, color_range)\n",
    "        \n",
    "    if viz == \"imshow\":\n",
    "        img = df.pivot_table(columns=\"epoch\", index=\"budget\", values=\"value\")\n",
    "        fig = px.imshow(img)\n",
    "        fig.update_yaxes(type='category')\n",
    "    elif viz == \"scatter\":\n",
    "        df = df.sort_values([\"epoch\", \"budget\"], ascending=[True, False])\n",
    "        df[\"budget\"] = df[\"budget\"].astype(\"str\")\n",
    "        df[\"epoch\"] = df[\"epoch\"].astype(\"str\")\n",
    "        fig = px.scatter(df, x=\"epoch\", y=\"budget\", size=\"count\", color=\"value\")\n",
    "    elif viz == \"effort\":\n",
    "        compute_effort(df)\n",
    "        key = 'log_effort'\n",
    "        x_res = 4000\n",
    "        partitions, x_index, interpolated = partitioned_interpolation(df, ['budget'], key, 'value', x_res)\n",
    "        fig= px.imshow(interpolated, aspect='auto', zmin=np.min(interpolated), zmax=np.max(interpolated),\n",
    "            x = x_index,\n",
    "            y = [str(b) for b in partitions[0][1]],\n",
    "            labels=dict(x=\"log(Effort)\", y=\"# Parameters\", color=z_label),)\n",
    "        fig.update_yaxes(type='category')\n",
    "    else:\n",
    "        return None\n",
    "    fig.update_layout(title=f\"{z_label} using {loss} for {dataset}, {topology}, residual {residual_mode}\")\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "656e7274f753432ea5f74bc21ac9bec4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='fixed_3k_1', description='group'), Dropdown(description='learning_rate', ind…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(**options, viz=[\"volume\", \"scatter\"])\n",
    "def heatmap_app_3d(group='fixed_3k_1',\n",
    "                   learning_rate=0.0001,\n",
    "                   label_noise = 0.0,\n",
    "                   dataset=\"201_pol\", \n",
    "                   topology=\"rectangle\",\n",
    "                   residual_mode=\"none\",\n",
    "                   agg=\"avg\", \n",
    "                   loss=\"val_loss\", \n",
    "                   viz=\"volume\", \n",
    "                   epoch_axis = 'epoch',\n",
    "                   color_range=1.25):\n",
    "    query_string = f'''\n",
    "    select budget, depth, {agg}(a.val) as value, count(a.val) as count, a.epoch\n",
    "    from\n",
    "        materialized_experiments_3_base base,\n",
    "        materialized_experiments_3_loss loss,\n",
    "        unnest(loss.{loss}) WITH ORDINALITY as a(val, epoch)\n",
    "    WHERE\n",
    "        base.id = loss.id and \n",
    "        groupname = {string_to_id(group)} and\n",
    "        dataset = {string_to_id(dataset)} and\n",
    "        learning_rate = {learning_rate}::real and\n",
    "        label_noise = {label_noise}::real and\n",
    "        topology = {string_to_id(topology)} and\n",
    "        residual_mode = {string_to_id(residual_mode)}\n",
    "    GROUP BY budget, depth, epoch\n",
    "    ORDER BY budget, depth, epoch;\n",
    "    '''\n",
    "#     df = cached_query(query_string).query(\"count >= 7\")    \n",
    "    df = cached_query(query_string)\n",
    "    value_label, minimizing, best = setup_value(df, loss, color_range)    \n",
    "    if 'effort' in epoch_axis:\n",
    "        compute_effort(df)        \n",
    "    \n",
    "    if viz==\"scatter\":        \n",
    "        return px.scatter_3d(df,\n",
    "                    x='config.depth',\n",
    "                    y='config.budget',\n",
    "                    z=epoch_axis,\n",
    "                    color='value',\n",
    "                    log_y=True,\n",
    "                    opacity=0.25)\n",
    "    elif viz == \"volume\":\n",
    "        x_res = 2000\n",
    "        partition_keys = ['budget', 'depth']\n",
    "        \n",
    "        if 'effort' in epoch_axis:\n",
    "            partitions, z_index, values = partitioned_interpolation(df, partition_keys, epoch_axis, 'value', x_res)\n",
    "        else:\n",
    "            partition_keys.append(epoch_axis)\n",
    "            partitions = get_values_for_categorical_keys(df, partition_keys)\n",
    "            z_index = np.array(list(partitions[2][1].keys()))\n",
    "            values = np.empty((len(partitions[0][2]), len(partitions[1][2]), len(partitions[2][2])))\n",
    "            values[:] = np.NaN\n",
    "            for _, row in df.iterrows():\n",
    "                values[partitions[0][1][row[partition_keys[0]]], \n",
    "                       partitions[1][1][row[partition_keys[1]]], \n",
    "                       partitions[2][1][row[partition_keys[2]]]] = row['value']\n",
    "        \n",
    "        max_value = np.nanmax(values)\n",
    "        values = np.nan_to_num(values, copy=False, nan=max_value + 1e-12) # set NaNs to just over the max value\n",
    "        \n",
    "        # trim data to the visible range\n",
    "        z_start = next((z for z in range(values.shape[2]) if np.any(values[:,:,z] < max_value)), 0)\n",
    "        z_end = next((z for z in reversed(range(values.shape[2])) if np.any(values[:,:,z] < max_value))\n",
    "                     , values.shape[2])\n",
    "        values = values[:,:,z_start:z_end+1]\n",
    "        z_index = z_index[z_start:z_end+1]\n",
    "            \n",
    "        print(f'value: {np.nanmin(values)} {np.nanmax(values)} z_start {z_start} z_end {z_end}')\n",
    "        mesh = [dim.flatten() for dim in np.meshgrid(\n",
    "            np.array(range(len(partitions[0][2]))),\n",
    "            np.array(range(len(partitions[1][2]))),\n",
    "            z_index,            \n",
    "            indexing='ij'\n",
    "            )]\n",
    "        print(f'{np.min(mesh[0])} {np.max(mesh[0])} {np.min(mesh[1])} {np.max(mesh[1])} {np.min(mesh[2])} {np.max(mesh[2])}')\n",
    "        fig = go.Figure(data=go.Volume(\n",
    "            x=mesh[0],\n",
    "            y=mesh[1],\n",
    "            z=mesh[2],\n",
    "            value=values.flatten(),\n",
    "            opacity=1.0,\n",
    "            isomin= np.nanmin(values),\n",
    "            isomax= max_value - 1e-12,\n",
    "            caps= dict(x_show=False, y_show=False, z_show=False),\n",
    "            surface_count=5,\n",
    "            opacityscale=\"min\",\n",
    "            colorscale=\"YlOrRd_r\"\n",
    "            ))\n",
    "        \n",
    "        print(z_index[0], z_index[-1])\n",
    "\n",
    "        axis_data = [dict(ticktext=p[2],\n",
    "                         tickvals=list(range(len(p[2]))),\n",
    "                         title=p[0]) \n",
    "                     for p in partitions]\n",
    "\n",
    "        fig.update_layout(\n",
    "            scene = dict(\n",
    "                xaxis = axis_data[0],\n",
    "                yaxis = axis_data[1],\n",
    "                zaxis = dict(\n",
    "                    nticks = 10,\n",
    "                    range=[z_index[0], z_index[-1]],\n",
    "                    title=epoch_axis\n",
    "                )\n",
    "            ),\n",
    "            width=950, height=950,\n",
    "            title=f\"{value_label} using {loss} for {dataset}, {topology}, residual {residual_mode}\",\n",
    "            )\n",
    "        return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51bbe8709f4e4593896bac6e521fffb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='fixed_3k_1', description='group'), Dropdown(description='learning_rate', ind…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@interact_manual(**options, \n",
    "                 statistic=[\"epoch\", \"effort\", \"log_effort\", 'relative_effort', 'relative_log_effort', 'relative_log_effort_to_minimize', 'best'], \n",
    "                 viz=['imshow', 'depth_lines', 'budget_lines'],\n",
    "                 depths=widgets.IntRangeSlider(min=2, max=20, step=1, value=(2,20)))\n",
    "def heatmap_app(group='fixed_3k_1',\n",
    "                learning_rate=0.0001,\n",
    "                label_noise = 0.0,\n",
    "                dataset=\"201_pol\", \n",
    "                topology=\"trapezoid\", \n",
    "                residual_mode=\"none\",\n",
    "                loss=\"history_val_loss\", \n",
    "                agg=\"avg\",\n",
    "                statistic = 'relative_log_effort', \n",
    "                color_range=1.10,\n",
    "                viz='imshow'):\n",
    "    \n",
    "    query_string = f'''\n",
    "    select budget, depth, {agg}(a.val) as value, count(a.val) as count, a.epoch\n",
    "    from\n",
    "        materialized_experiments_3_base base,\n",
    "        materialized_experiments_3_loss loss,\n",
    "        unnest(loss.{loss}) WITH ORDINALITY as a(val, epoch)\n",
    "    WHERE\n",
    "        base.id = loss.id and \n",
    "        groupname = {string_to_id(group)} and\n",
    "        dataset = {string_to_id(dataset)} and\n",
    "        learning_rate = {learning_rate}::real and\n",
    "        label_noise = {label_noise}::real and\n",
    "        topology = {string_to_id(topology)} and\n",
    "        residual_mode = {string_to_id(residual_mode)}\n",
    "    GROUP BY budget, depth, epoch\n",
    "    ORDER BY budget, depth, epoch;\n",
    "    '''\n",
    "    print('Querying...')\n",
    "    df = cached_query(query_string)\n",
    "    print('Done.')\n",
    "#     z_label, minimizing, best = setup_value(df, loss, color_range)    \n",
    "        \n",
    "#     if viz==\"efficiency\":        \n",
    "#         z_label = setup_value(df, loss, color_range)\n",
    "    partition_keys = ['depth', 'budget']\n",
    "    partitions = get_values_for_categorical_keys(df, partition_keys)\n",
    "    \n",
    "    filtered = df\n",
    "    if statistic != 'relative_log_effort_to_minimize':\n",
    "        best = np.nanmin(filtered['value']) # find best value\n",
    "        threshold = best * color_range\n",
    "        filtered = filtered[filtered['value'] <= threshold]\n",
    "            \n",
    "    minimize = 'epoch'\n",
    "    select = 'epoch'\n",
    "    if statistic == 'relative_log_effort_to_minimize':\n",
    "        minimize = 'value'\n",
    "    elif statistic == 'best':\n",
    "        minimize = 'value'\n",
    "        select = 'value'\n",
    "\n",
    "    def find_first_epoch(group):\n",
    "        return group.sort_values(minimize).iloc[0][select]\n",
    "\n",
    "    composite = filtered.groupby(partition_keys).apply(find_first_epoch).reset_index(name=select)\n",
    "\n",
    "    z_label = statistic\n",
    "    if statistic == 'best':\n",
    "        z_label, minimizing, best = setup_value(composite, loss, color_range)\n",
    "        composite[statistic] = composite[select] / np.min(composite[select])\n",
    "    else:\n",
    "        compute_effort(composite)\n",
    "        if statistic == 'relative_log_effort_to_minimize':\n",
    "            composite['relative_log_effort_to_minimize'] = composite['relative_log_effort']\n",
    "    \n",
    "    color_continuous_scale = 'Greens_r'\n",
    "    if statistic in ['epoch']:\n",
    "        color_continuous_scale = 'Sunsetdark'\n",
    "\n",
    "    if viz == 'imshow':\n",
    "        x_key = 'depth'\n",
    "        y_key = 'budget'\n",
    "        img = composite.pivot_table(columns=x_key, index=y_key, values=statistic)\n",
    "        fig = px.imshow(img, color_continuous_scale=color_continuous_scale,\n",
    "                       labels=dict(x=x_key, y=y_key, color=z_label))\n",
    "        axis_data = [dict(\n",
    "            categoryorder='array',\n",
    "            categoryarray=p[2],\n",
    "            title=p[0],\n",
    "            type='category') \n",
    "            for p in partitions]    \n",
    "        fig.update_layout(\n",
    "            xaxis = axis_data[0],\n",
    "            yaxis = axis_data[1],\n",
    "            title=f'{dataset}, {topology}, residual {residual_mode}<br>{statistic} to reach {\"{:.2f}\".format(100*(color_range-1))}% of best {loss}',\n",
    "#             coloraxis=dict(\n",
    "#                 title=statistic\n",
    "#                 )\n",
    "            coloraxis_colorbar_x=.7,\n",
    "            )\n",
    "    elif viz == 'depth_lines':\n",
    "        fig = px.line(composite, x='budget', y=statistic, color='depth', log_x=True)\n",
    "        \n",
    "    elif viz == 'budget_lines':\n",
    "        fig = px.line(composite, x='depth', y=statistic, color='budget')\n",
    "    return fig\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94063676e27742f5833f1029ab12f7f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Text(value='fixed_3k_1', description='group'), Dropdown(description='learning_rate', ind…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Heapmap App using Imshow\n",
    "\n",
    "@interact_manual(**options, \n",
    "                 viz=[\"imshow\", \"scatter\", \"effort\"], \n",
    "                 depths=widgets.IntRangeSlider(min=2, max=20, step=1, value=(2,20)))\n",
    "def heatmap_app(\n",
    "    group='fixed_3k_1',\n",
    "    learning_rate=0.0001,\n",
    "    label_noise = 0.0,\n",
    "    dataset=\"201_pol\", \n",
    "    topology=\"trapezoid\", \n",
    "    residual_mode=\"none\",\n",
    "    loss=\"val_loss\", \n",
    "    agg=\"avg\",\n",
    "    viz=\"effort\", \n",
    "    depths=(2,20),\n",
    "    color_range=1.3):\n",
    "    \n",
    "    query_string = f'''\n",
    "    select budget, {agg}(a.val) as value, count(a.val) as count, a.epoch\n",
    "    from\n",
    "        materialized_experiments_3_base base,\n",
    "        materialized_experiments_3_loss loss,\n",
    "        unnest(loss.{loss}) WITH ORDINALITY as a(val, epoch)\n",
    "    WHERE\n",
    "        base.id = loss.id and \n",
    "        groupname = {string_to_id(group)} and\n",
    "        dataset = {string_to_id(dataset)} and\n",
    "        learning_rate = {learning_rate}::real and\n",
    "        label_noise = {label_noise}::real and\n",
    "        topology = {string_to_id(topology)} and\n",
    "        residual_mode = {string_to_id(residual_mode)} and\n",
    "        depth BETWEEN {depths[0]} and {depths[1]}\n",
    "    GROUP BY budget, epoch\n",
    "    ORDER BY budget, epoch;\n",
    "    '''\n",
    "    print('Querying...')\n",
    "    df = cached_query(query_string)\n",
    "    print('done.')\n",
    "    return make_2d_heatmap_viz(df, group, dataset, topology, loss, agg, residual_mode, viz, color_range)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
